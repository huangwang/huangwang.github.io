<!DOCTYPE html>






  


<html class="theme-next muse use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.2.0" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.2.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.2.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.2.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.2.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.2.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="keywords" content="Blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Jack Huang&#39;s Blog">
<meta property="og:url" content="https://huangwang.github.io/page/12/index.html">
<meta property="og:site_name" content="Jack Huang&#39;s Blog">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Jack Huang&#39;s Blog">



  <link rel="alternate" href="/atom.xml" title="Jack Huang's Blog" type="application/atom+xml">




  <link rel="canonical" href="https://huangwang.github.io/page/12/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Jack Huang's Blog</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jack Huang's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>




<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>
  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangwang.github.io/2018/12/05/三维建模方法简介/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Huang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Huang's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/05/三维建模方法简介/" itemprop="url">
                  三维建模方法简介
                </a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-12-05 21:17:42 / 修改时间：21:39:26" itemprop="dateCreated datePublished" datetime="2018-12-05T21:17:42+08:00">2018-12-05</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>三维建模是一种常用技术，奈何心向往之，却没有时间去系统学习，也没有做出过一个作品。下面将我看到的一些好的有关三维建模的资料记录一下，以做备忘。</p>
<h1 id="三维建模方法"><a href="#三维建模方法" class="headerlink" title="三维建模方法"></a>三维建模方法</h1><h2 id="选择软件"><a href="#选择软件" class="headerlink" title="选择软件"></a>选择软件</h2><p>当前三维软件层出不穷，既有商业级的3ds MAX、MAYA、Creator，也有免费使用的Blender、MilkShape。</p>
<h2 id="下载蓝图"><a href="#下载蓝图" class="headerlink" title="下载蓝图"></a>下载蓝图</h2><p>飞行仿真的三维模型主要有飞行器、机场建筑等。和大多数虚拟现实中使用的模型一样，并不需要CAD级别的精度，一般是下载一些图片或飞机整体蓝图作为蓝本，采用多边形建模的方法分部建模。蓝图下载网址首推：</p>
<p><a href="http://www.the-blueprints.com/" target="_blank" rel="noopener">the‑blueprints.com</a></p>
<p>这里免费提供各种汽车、国内外飞机、船舶的三视图图纸下载。</p>
<h2 id="多边形建模"><a href="#多边形建模" class="headerlink" title="多边形建模"></a>多边形建模</h2><p>采用下载的蓝图，作为贴图放置在相应的投影平面上，然后采用多边形建模的方法画出各部分细节。多边形建模的思想就是采用一个诸如一个最接近的多边形，进行点线面的操作就可以绘制出复杂的飞机外形。</p>
<p>多边形建模从最基本的外形开始变形，可以从点线面及整体四个级别的操作，在越需要突出外形细节处用越多的点，通过建立网格模型，然后使用网格平滑功能生成平滑的曲面模型。</p>
<p>建模时不能有重复的点线面等几何体，否则即使表面上已经贴好了材质，但载入程序后，往往会因为Z向深度冲突的原因产生闪烁效果。其细节取决于您的需求，需要花费大量的时间。</p>
<h2 id="贴图"><a href="#贴图" class="headerlink" title="贴图"></a>贴图</h2><p>真实世界的模型带有各种彩色并具有光泽度。完成了几何体的建模对于一个模型来说只完成了一小半工作，还需要给模型添加材质。</p>
<p>与一般的模型不同，飞行器出于气动设计的考虑，其表面模型一般成流线型的曲面，且部件组成复杂，界线较难区分，即使是UVW贴图也难以胜任。因此，对飞行器的贴图主要采用3ds MAX的UVW展开贴图技术贴图。</p>
<p>一个模型最好只有一个贴图，因为贴图数目过多零散会影响计算机载入三维模型时间和效率。因此，一般将整个模型各个部件组合为一个多边形，各部件为元素，待完成UVW贴图后再分离成单独的部件。UVW展开贴图有多重形式，如长方体展开（即按六个视图投影）、平面展开等，按需选择。</p>
<p>在一张贴图上展开整个模型的UV坐标显然是有限的，需要根据现实精度调整各个部分的贴图大小。保存这些展开的贴图坐标并渲染输出UV模板图像文件。UV模板图像其实就是整个模型的贴图的界线图像，通过Photoshop等图像处理软件处理，采用尽可能真实的图像放置在对应的区域。最后将材质赋给对象即完成贴图渲染。</p>
<h2 id="数据导出"><a href="#数据导出" class="headerlink" title="数据导出"></a>数据导出</h2><p>直接采用3ds MAX绘制的三维模型当然是<em>.max格式，它不能为外界程序读取。这时就需要根据自己的开发环境选择导出的数据格式，比如用开源的OSG做仿真开发，就需要OSGExp插件的支持，导成</em>.ive、<em>.osg、</em>.osgb等格式即可。但无论导出何种格式，<strong>切记保证模型质心与建模的原点不能相差不远，还要注意光照、贴图格式以及单位问题</strong>。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/34555581" target="_blank" rel="noopener">飞行仿真—1.三维建模篇</a>,by WFYX.</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangwang.github.io/2018/11/26/深度强化学习研究笔记/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Huang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Huang's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/26/深度强化学习研究笔记/" itemprop="url">
                  深度强化学习研究笔记
                </a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-26 10:07:07" itemprop="dateCreated datePublished" datetime="2018-11-26T10:07:07+08:00">2018-11-26</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-01-17 21:50:13" itemprop="dateModified" datetime="2019-01-17T21:50:13+08:00">2019-01-17</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>深度强化学习Deep Reinforcement Learning是将深度学习与强化学习结合起来，从而实现从Perception感知到Action动作的端对端学习End-to-End Learning的一种全新的算法。深度强化学习具备使机器人实现真正完全自主地学习一种甚至多种技能的潜力。</p>
<h1 id="深度强化学习的起源"><a href="#深度强化学习的起源" class="headerlink" title="深度强化学习的起源"></a>深度强化学习的起源</h1><p>深度强化学习是深度学习与强化学习相结合的产物。</p>
<h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><p>深度学习（deep learning）是机器学习的分支，是一种试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。</p>
<p>深度学习是机器学习中一种基于对数据进行表征学习的算法。观测值（例如一幅图像）可以使用多种方式来表示，如每个像素强度值的向量，或者更抽象地表示成一系列边、特定形状的区域等。而使用某些特定的表示方法更容易从实例中学习任务（例如，人脸识别或面部表情识别）。深度学习的好处是用非监督式或半监督式的特征学习和分层特征提取高效算法来替代手工获取特征。</p>
<p>表征学习的目标是寻求更好的表示方法并创建更好的模型来从大规模未标记数据中学习这些表示方法。表示方法来自神经科学，并松散地创建在类似神经系统中的信息处理和对通信模式的理解上，如神经编码，试图定义拉动神经元的反应之间的关系以及大脑中的神经元的电活动之间的关系。</p>
<p>至今已有数种深度学习框架，如深度神经网络、卷积神经网络和深度置信网络和递归神经网络已被应用在计算机视觉、语音识别、自然语言处理、音频识别与生物信息学等领域并获取了极好的效果。</p>
<p>另外，“深度学习”已成为类似术语，或者说是神经网络的品牌重塑。</p>
<h3 id="深度神经网络"><a href="#深度神经网络" class="headerlink" title="深度神经网络"></a>深度神经网络</h3><p>深度神经网络是一种具备至少一个隐层的神经网络。与浅层神经网络类似，深度神经网络也能够为复杂非线性系统提供建模，但多出的层次为模型提供了更高的抽象层次，因而提高了模型的能力。</p>
<p>深度神经网络（Deep Neural Networks, DNN）是一种判别模型，可以使用反向传播算法进行训练。权重更新可以使用下式进行随机梯度下降法求解：</p>
<script type="math/tex; mode=display">\Delta w_{ij}(t+1)=\Delta w_{ij}(t) +  \eta\frac{\partial C}{\partial w_{ij}}</script><p>其中，$\eta$为学习率，$C$为代价函数。这一函数的选择与学习的类型（例如监督学习、无监督学习、增强学习）以及激活函数相关。例如，为了在一个多分类问题上进行监督学习，通常的选择是使用ReLU作为激活函数，而使用交叉熵作为代价函数。Softmax函数定义为${\displaystyle p<em>{j}={\frac {\exp(x</em>{j})}{\sum <em>{k}\exp(x</em>{k})}}}$，其中 ${\displaystyle p<em>{j}}$代表类别 ${\displaystyle j}$的概率，而 ${\displaystyle x</em>{j}}$和 ${\displaystyle x<em>{k}}$分别代表对单元 ${\displaystyle j}$ 和 ${\displaystyle k}$的输入。交叉熵定义为 $C = -\sum_j d_j \log(p_j)$ ，其中 ${\displaystyle d</em>{j}}$代表输出单元${\displaystyle j}$的目标概率， ${\displaystyle p_{j}}$代表应用了激活函数后对单元 ${\displaystyle j}$的概率输出。</p>
<h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><p>在人工智能领域，一般用智能体Agent表示一个具备行为能力的物体，比如机器人，无人车等等。而强化学习则研究智能体Agent和环境Environment之间交互过程如何取得任务的成功。</p>
<p>强化学习与环境的交互过程如图1所示。在某个时间点，智能体Agent会获得观察值(Observation)和反馈值(Reward)，然后根据这些选择下一步的动作(Action)。</p>
<p></p><p align="center">
    <img src="images/reinforce_learning.png" width="90%" alt="强化学习示意图">
</p><p></p>
<center>图1 强化学习示意图</center>

<p>在整个过程中，任务的目标是获取尽可能多的Reward，这是任务的目标。而在每个时间片，Agent都是根据当前的观察来确定下一步的动作。观察Observation的集合就作为Agent的所处的状态State，因此，状态State和动作Action存在映射关系，也就是一个state可以对应一个action，或者对应不同动作的概率（常常用概率来表示，概率最高的就是最值得执行的动作）。状态与动作的关系其实就是输入与输出的关系，而状态State到动作Action的过程就称之为一个策略Policy，一般用 $\pi$  表示，也就是需要找到以下关系：</p>
<script type="math/tex; mode=display">a=\pi(s)</script><p>或者</p>
<script type="math/tex; mode=display">\pi(a|s)</script><p>其中a是action，s是state。第一种是一一对应的表示，第二种是概率的表示。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">强化学习的任务就是找到一个最优的策略Policy从而使Reward最多。</span><br></pre></td></tr></table></figure></p>
<p>强化学习的训练一开始从采用随机策略进行试验开始，可获得一系列的状态,动作和反馈：</p>
<script type="math/tex; mode=display">\{s_1,a_1,r_1,s_2,a_2,r_2,...,s_t,a_t,r_t\}</script><p>根据这一系列样本，强化学习从中改进策略，使得任务反馈Reward越来越多。</p>
<h3 id="强化学习模型假设"><a href="#强化学习模型假设" class="headerlink" title="强化学习模型假设"></a>强化学习模型假设</h3><p>强化学习的研究建立在经典物理学基础上，基于以下两种假设：</p>
<ol>
<li>时间是可以分割成一个一个时间片的，并且有完全的先后顺序。</li>
<li>上帝不掷筛子！如果输入是确定的，那么输出也一定是确定的。</li>
</ol>
<h3 id="强化学习与马尔科夫决策过程"><a href="#强化学习与马尔科夫决策过程" class="headerlink" title="强化学习与马尔科夫决策过程"></a>强化学习与马尔科夫决策过程</h3><p>MDP基于这样一种假设：未来只取决于当前。</p>
<p>一个状态$S_t$是Markov当且仅当</p>
<script type="math/tex; mode=display">P(s_{t+1}|s_t)=P(s_{t+1}|s_t,s_{t-1},...s_1,s_0)</script><p>P为概率。简单的说就是下一个状态仅取决于当前的状态和当前的动作。注意这里的状态是完全可观察的全部的环境状态（也就是上帝视角）。</p>
<p>增强学习的问题都可以模型化为MDP的问题。</p>
<p>一个基本的MDP可以用（S,A,P）来表示，S表示状态，A表示动作，P表示状态转移概率，也就是根据当前的状态$s<em>t$和$a_t$转移到$s</em>{t+1}$的概率。如果我们知道了转移概率P，也就是称为我们获得了模型Model，有了模型，未来就可以求解，那么获取最优的动作也就有可能，这种通过模型来获取最优动作的方法也就称为Model-based的方法。但是现实情况下，很多问题是很难得到准确的模型的，因此就有Model-free的方法来寻找最优的动作。</p>
<h3 id="强化学习的回报Result"><a href="#强化学习的回报Result" class="headerlink" title="强化学习的回报Result"></a>强化学习的回报Result</h3><p>既然一个状态对应一个动作，或者动作的概率，而有了动作，下一个状态也就确定了。这就意味着每个状态可以用一个确定的值来进行描述。可以由此判断一个状态是好的状态还是不好的状态。</p>
<p>状态的好坏其实等价于对未来回报的期望。因此，引入回报Return来表示某个时刻t的状态将具备的回报：</p>
<script type="math/tex; mode=display">G_t = R_{t+1} + \lambda R_{t+2} + ... = \sum_{k=0}^\infty\lambda^kR_{t+k+1}</script><p>上面R是Reward反馈，λ是discount factor折扣因子，一般小于1，就是说一般当下的反馈是比较重要的，时间越久，影响越小。</p>
<p>那么实际上除非整个过程结束，否则显然我们无法获取所有的reward来计算出每个状态的Return，因此，再引入一个概念价值函数Value Function,用value function $v(s)$来表示一个状态未来的潜在价值。</p>
<p>从定义上看，value function就是回报的期望：</p>
<script type="math/tex; mode=display">v(s) = \mathbb E[G_t|S_t = s]</script><p>引出价值函数，对于获取最优的策略Policy这个目标，我们就会有两种方法：</p>
<ul>
<li>直接优化策略$\pi(a|s)$或者$a = \pi(s)$使得回报更高</li>
<li>通过估计value function来间接获得优化的策略。道理很简单，既然我知道每一种状态的优劣，那么我就知道我应该怎么选择了，而这种选择就是我们想要的策略。</li>
</ul>
<p>把值函数展开：</p>
<script type="math/tex; mode=display">\begin{aligned}
    v(s) & = \mathbb E[G_t|S_t = s] \\\\
      & = \mathbb E[R_{t+1}+\lambda R_{t+2} + \lambda ^2R_{t+3} + ...|S_t = s] \\\\ 
      & = \mathbb E[R_{t+1}+\lambda (R_{t+2} + \lambda R_{t+3} + ...)|S_t = s] \\\\
      & = \mathbb E[R_{t+1} + \lambda G_{t+1}|S_t = s] \\\\ 
      & = \mathbb E[R_{t+1} + \lambda v(S_{t+1})|S_t = s]
\end{aligned}</script><p>因此：</p>
<script type="math/tex; mode=display">v(s) = \mathbb E[R_{t+1} + \lambda v(S_{t+1})|S_t = s]</script><p>上面这个公式就是Bellman方程的基本形态。从公式上看，当前状态的价值和下一步的价值以及当前的反馈Reward有关。它表明Value Function是可以通过迭代来进行计算的!!!</p>
<h3 id="Action-Value-function-动作价值函数"><a href="#Action-Value-function-动作价值函数" class="headerlink" title="Action-Value function 动作价值函数"></a>Action-Value function 动作价值函数</h3><p>前面我们引出了价值函数，考虑到每个状态之后都有多种动作可以选择，每个动作之下的状态又多不一样，我们更关心在某个状态下的不同动作的价值。显然。如果知道了每个动作的价值，那么就可以选择价值最大的一个动作去执行了。这就是Action-Value function  $Q^\pi(s,a)$。那么同样的道理，也是使用reward来表示，只是这里的reward和之前的reward不一样，这里是执行完动作action之后得到的reward，之前state对应的reward则是多种动作对应的reward的期望值。显然，动作之后的reward更容易理解。</p>
<p>动作价值函数就为如下表示：</p>
<script type="math/tex; mode=display">\begin{aligned}
Q^\pi(s,a) & =  \mathbb E[r_{t+1} + \lambda r_{t+2} + \lambda^2r_{t+3} + ... |s,a] \\\\
& = \mathbb E_{s^\prime}[r+\lambda Q^\pi(s^\prime,a^\prime)|s,a]
\end{aligned}</script><p>这里要说明的是动作价值函数的定义，加了$\pi$,也就是说是在策略下的动作价值。因为对于每一个动作而已，都需要由策略根据当前的状态生成，因此必须有策略的支撑。而前面的价值函数则不一定依赖于策略。当然，如果定义$v^\pi(s)$则表示在策略$\pi$下的价值。</p>
<h3 id="Optimal-value-function-最优价值函数"><a href="#Optimal-value-function-最优价值函数" class="headerlink" title="Optimal value function 最优价值函数"></a>Optimal value function 最优价值函数</h3><p>能计算动作价值函数是不够的，因为我们需要的是最优策略，现在求解最优策略等价于求解最优的value function，找到了最优的value function，自然而然策略也就是找到。（当然，这只是求解最优策略的一种方法，也就是value-based approach，由于DQN就是value-based，因此这里只讲这部分，以后我们会看到还有policy-based和model-based方法。一个就是直接计算策略函数，一个是估计模型，也就是计算出状态转移函数，从而整个MDP过程得解）</p>
<p>这里以动作价值函数来分析。</p>
<p>首先是最优动作价值函数和一般的动作价值函数的关系：</p>
<script type="math/tex; mode=display">\begin{aligned}
Q^*(s,a) &= \max_\pi Q^\pi(s,a)\\\\
 & = \mathbb E_{s^\prime}[r+\lambda \max _{a^\prime}Q^*(s^\prime,a^\prime)|s,a]
\end{aligned}</script><p>也就是最优的动作价值函数就是所有策略下的动作价值函数的最大值。通过这样的定义就可以使最优的动作价值的唯一性，从而可以求解整个MDP。</p>
<p>下面介绍基于Bellman方程的两个最基本的算法，策略迭代和值迭代。</p>
<h3 id="策略迭代Policy-Iteration"><a href="#策略迭代Policy-Iteration" class="headerlink" title="策略迭代Policy Iteration"></a>策略迭代Policy Iteration</h3><p>Policy Iteration的目的是通过迭代计算value function 价值函数的方式来使policy收敛到最优。</p>
<p>Policy Iteration本质上就是直接使用Bellman方程而得到的：</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{k+1}(s) &\overset{.}{=} \mathbb E_{\pi}[R_{t+1}+\gamma v _{k}(S_{t+1})|S_t=s]\\\\
&=\sum_{a}{\pi(a|s)}\sum_{s^\prime,r}{p(s^\prime,r|s,a)[r+\gamma v_k(s^\prime)]}
\end{aligned}</script><p>那么Policy Iteration一般分成两步：</p>
<ol>
<li>Policy Evaluation 策略评估。目的是更新Value Function，或者说更好的估计基于当前策略的价值</li>
<li>Policy Improvement 策略改进。 使用 greedy policy 产生新的样本用于第一步的策略评估。</li>
</ol>
<p></p><p align="center">
    <img src="images/policy_iteration.png" width="90%" alt="策略迭代算法示意图">
</p><p></p>
<center>图2 策略迭代算法示意图</center>

<p>本质上就是使用当前策略产生新的样本，然后使用新的样本更好的估计策略的价值，然后利用策略的价值更新策略，然后不断反复。理论可以证明最终策略将收敛到最优。</p>
<p>具体算法：</p>
<p></p><p align="center">
    <img src="images/policy_iteration_method.png" width="90%" alt="策略迭代算法">
</p><p></p>
<center>图3 策略迭代算法</center>

<p>那么这里要注意的是policy evaluation部分。这里的迭代很重要的一点是需要知道state状态转移概率p。也就是说依赖于model模型。而且按照算法要反复迭代直到收敛为止。所以一般需要做限制。比如到某一个比率或者次数就停止迭代。那么需要特别说明的是不管是策略迭代还是值迭代都是在理想化的情况下（上帝视角）推导出来的算法，本质上并不能直接应用，因为依赖Model。</p>
<h3 id="Value-Iteration-价值迭代"><a href="#Value-Iteration-价值迭代" class="headerlink" title="Value Iteration 价值迭代"></a>Value Iteration 价值迭代</h3><p>Value Iteration则是使用Bellman 最优方程得到:</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{*}(s) & = \max_{a} \mathbb E_{\pi}[R_{t+1}+\gamma v _{*}(S_{t+1})|S_t=s,A_t=a]\\\\
& = \max_{a} \sum_{s^\prime,r}p(s^\prime,r|s,a)[r+\gamma v_*(s^\prime)]
\end{aligned}</script><p>然后改变成迭代形式:</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{k+1}(s) &\overset{.}{=} \max_{a} \mathbb E_{\pi}[R_{t+1}+\gamma v _{k}(S_{t+1})|S_t=s,A_t=a]\\\\
& = \max_{a} \sum_{s^\prime,r}p(s^\prime,r|s,a)[r+\gamma v_k(s^\prime)]
\end{aligned}</script><p>value iteration的算法如下：</p>
<p></p><p align="center">
    <img src="images/policy_iteration_method.png" width="90%" alt="值迭代算法">
</p><p></p>
<center>图4 值迭代算法</center>

<p>Policy Iteration和Value Iteration有什么本质区别？</p>
<p>policy iteration使用bellman方程来更新value，最后收敛的value 即$v_\pi$是当前policy下的value值（所以叫做对policy进行评估），目的是为了后面的policy improvement得到新的policy。</p>
<p>而value iteration是使用bellman 最优方程来更新value，最后收敛得到的value即$v_*$就是当前state状态下的最优的value值。因此，只要最后收敛，那么最优的policy也就得到的。因此这个方法是基于更新value的，所以叫value iteration。</p>
<p>从上面的分析看，value iteration较之policy iteration更直接。不过问题也都是一样，需要知道状态转移函数p才能计算。本质上依赖于模型，而且理想条件下需要遍历所有的状态，这在稍微复杂一点的问题上就基本不可能了。</p>
<h3 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h3><p>Q Learning的思想完全根据value iteration得到。但要明确一点是value iteration每次都对所有的Q值更新一遍，也就是所有的状态和动作。但事实上在实际情况下我们没办法遍历所有的状态，还有所有的动作，我们只能得到有限的系列样本。因此，只能使用有限的样本进行操作。那么，怎么处理？Q Learning提出了一种更新Q值的办法：</p>
<script type="math/tex; mode=display">Q(S_{t},A_{t}) \leftarrow Q(S_{t},A_{t})+\alpha({R_{t+1}+\lambda \max _aQ(S_{t+1},a)} - Q(S_t,A_t))</script><p>虽然根据value iteration计算出target Q值，但是这里并没有直接将这个Q值（是估计值）直接赋予新的Q，而是采用渐进的方式类似梯度下降，朝target迈近一小步，取决于α,这就能够减少估计误差造成的影响。类似随机梯度下降，最后可以收敛到最优的Q值。</p>
<p>具体的算法如下：</p>
<p></p><p align="center">
    <img src="images/policy_iteration_method.png" width="90%" alt="Q学习算法">
</p><p></p>
<center>图5 Q学习算法</center>

<h3 id="Exploration-and-Exploitation-探索与利用"><a href="#Exploration-and-Exploitation-探索与利用" class="headerlink" title="Exploration and Exploitation 探索与利用"></a>Exploration and Exploitation 探索与利用</h3><p>回到policy的问题，那么要选择怎样的policy来生成action呢？有两种做法：</p>
<ul>
<li><p>随机的生成一个动作</p>
</li>
<li><p>根据当前的Q值计算出一个最优的动作，这个policy\pi称之为greedy policy贪婪策略。也就是</p>
</li>
</ul>
<script type="math/tex; mode=display">\pi(S_{t+1}) = arg\max _aQ(S_{t+1},a)</script><p>使用随机的动作就是exploration，也就是探索未知的动作会产生的效果，有利于更新Q值，获得更好的policy。而使用greedy policy也就是target policy则是exploitation，利用policy，这个相对来说就不好更新出更好的Q值，但可以得到更好的测试效果用于判断算法是否有效。</p>
<p>将两者结合起来就是所谓的$\epsilon-greedy$策略，$\epsilon$一般是一个很小的值，作为选取随机动作的概率值。可以更改$\epsilon$的值从而得到不同的exploration和exploitation的比例。</p>
<p>这里需要说明的一点是使用$\epsilon-greedy$策略是一种极其简单粗暴的方法，对于一些复杂的任务采用这种方法来探索未知空间是不可取的。因此，最近有越来越多的方法来改进这种探索机制。</p>
<h2 id="深度强化学习"><a href="#深度强化学习" class="headerlink" title="深度强化学习"></a>深度强化学习</h2><p>深度强化学习始于DeepMind在NIPS 2013上发表的Playing Atari with Deep Reinforcement Learning一文，在该文中第一次提出Deep Reinforcement Learning 这个名称，并且提出DQN（Deep Q-Network）算法，实现从纯图像输入完全通过学习来玩Atari游戏的成果。</p>
<p>之后DeepMind在Nature上发表了改进版的DQN文章Human-level Control through Deep Reinforcement Learning，引起了广泛的关注，Deep Reinfocement Learning 从此成为深度学习领域的前沿研究方向。</p>
<h3 id="维度灾难"><a href="#维度灾难" class="headerlink" title="维度灾难"></a>维度灾难</h3><p>对简单问题可使用表格来表示Q(s,a)，但是这个在现实的很多问题上是几乎不可行的，因为状态实在是太多。使用表格的方式根本存不下。</p>
<p>以计算机玩Atari游戏为例，计算机玩Atari游戏的要求是输入原始图像数据，也就是210x160像素的图片，然后输出几个按键动作。总之就是和人类的要求一样，纯视觉输入，然后让计算机自己玩游戏。那么这种情况下，到底有多少种状态呢？有可能每一秒钟的状态都不一样。因为，从理论上看，如果每一个像素都有256种选择，那么就有：</p>
<script type="math/tex; mode=display">
256^{210\times 160}</script><p>这简直是天文数字。所以，我们是不可能通过表格来存储状态的。我们有必要对状态的维度进行压缩，解决办法就是 价值函数近似Value Function Approximation</p>
<h3 id="价值函数近似Value-Function-Approximation"><a href="#价值函数近似Value-Function-Approximation" class="headerlink" title="价值函数近似Value Function Approximation"></a>价值函数近似Value Function Approximation</h3><p>什么是价值函数近似呢？说起来很简单，就是用一个函数来表示Q(s,a)。即</p>
<script type="math/tex; mode=display">Q(s,a) = f(s,a)</script><p>$f$可以是任意类型的函数，比如线性函数：</p>
<script type="math/tex; mode=display">Q(s,a) = w_1s + w_2a + b</script><p>其中$w_1$,$w_2$,$b$是函数$f$的参数。</p>
<p>通过函数表示，我们就可以无所谓s到底是多大的维度，反正最后都通过矩阵运算降维输出为单值的Q。</p>
<p>这就是价值函数近似的基本思路。</p>
<p>如果我们就用$w$来统一表示函数$f$的参数，那么就有</p>
<script type="math/tex; mode=display">Q(s,a) = f(s,a,w)</script><p>为什么叫近似，因为我们并不知道Q值的实际分布情况，本质上就是用一个函数来近似Q值的分布，所以，也可以说是</p>
<script type="math/tex; mode=display">Q(s,a)\approx f(s,a,w)</script><h3 id="Q值神经网络化——DQN算法"><a href="#Q值神经网络化——DQN算法" class="headerlink" title="Q值神经网络化——DQN算法"></a>Q值神经网络化——DQN算法</h3><p>意思很清楚，就是我们用一个深度神经网络来表示这个函数$f$。</p>
<p>以DQN为例，输入是经过处理的4个连续的84x84图像，然后经过两个卷积层，两个全连接层，最后输出包含每一个动作Q值的向量。</p>
<p>神经网络的训练是一个最优化问题，最优化一个损失函数loss function，也就是标签和网络输出的偏差，目标是让损失函数最小化。为此，我们需要有样本，巨量的有标签数据，然后通过反向传播使用梯度下降的方法来更新神经网络的参数。</p>
<p>如何为Q网络提供有标签的样本？答案就是利用Q-Learning算法。目标Q值作为标签，使Q值趋近于目标Q值。于是Q网络训练的损失函数就是：</p>
<p></p><p align="center">
    <img src="images/loss_function.png" width="80%" alt="DQN损失函数">
</p><p></p>
<center>图6 DQN损失函数</center>

<p>上面公式是$s^\prime$, $a^\prime$即下一个状态和动作。这里用了David Silver的表示方式，看起来比较清晰。<br>既然确定了损失函数，也就是cost，确定了获取样本的方式。那么DQN的整个算法也就成型了！</p>
<p>这里分析第一个版本的DQN，也就是NIPS 2013提出的DQN。</p>
<p></p><p align="center">
    <img src="images/dqn_method.png" width="90%" alt="DQN算法">
</p><p></p>
<center>图7 DQN算法</center>

<p>具体的算法主要涉及到Experience Replay，也就是经验池的技巧，就是如何存储样本及采样问题。</p>
<p>由于玩Atari采集的样本是一个时间序列，样本之间具有连续性，如果每次得到样本就更新Q值，受样本分布影响，效果会不好。因此，一个很直接的想法就是把样本先存起来，然后随机采样如何？这就是Experience Replay的意思。按照脑科学的观点，人的大脑也具有这样的机制，就是在回忆中学习。</p>
<p>那么上面的算法看起来那么长，其实就是反复试验，然后存储数据。接下来数据存到一定程度，就每次随机采用数据，进行梯度下降！</p>
<h3 id="策略梯度"><a href="#策略梯度" class="headerlink" title="策略梯度"></a>策略梯度</h3><p>Policy Gradient的方法的基本思想是通过评价动作action的好坏，来调整该action的出现概率。最基本的Policy Gradient的损失函数Loss就是：</p>
<script type="math/tex; mode=display">loss = -log(\pi)*Q</script><p>这里先以Q值来指代对动作的评价。</p>
<h1 id="Robot-Learning的发展路径"><a href="#Robot-Learning的发展路径" class="headerlink" title="Robot Learning的发展路径"></a>Robot Learning的发展路径</h1><p>Robot Learning从目前来看，经过了以下研究思路的发展：</p>
<p>（1）利用传统的控制算法结合深度学习来实现机器人端到端的控制。这个方法主要是以Guided Policy Search（GPS）为首。这个方法是Sergey Levine提出的，通过与传统方法结合，确实可以让机器人学习出一些有意思的技能，但是有个根本问题摆在面前，就是传统方法通常需要知道整个系统的模型，而这在实际的机器人中非常难以适用。就比如四轴飞行器的控制，我们可以通过外部的Vicon设备来精确的定位四轴飞行器的位置，从而实现对其精确控制，但是在户外，我们根本就做不到这点，也就无法精确建模。因此，还依赖传统方法是没有出路的，我们使用深度学习就是要抛弃传统方法的弊端。</p>
<p>（2）深度增强学习DRL。由于DeepMind在DRL取得了巨大成功，而DRL就是面向决策与控制问题，特别适用于机器人，因此想在机器人上使用DRL是一种必然的想法。Google Brain团队（依然以Sergey Levine为首）做出了一些进展，在我们之前的专栏文章中也有分析最前沿 之 谷歌的协作机械臂 - 知乎专栏 。但是在使用DRL之后，DRL的弊端也就显现出来了，那就是需要大量的尝试来获取数据。对于这个问题，在机器人仿真环境还好，但是在真实的机器人上就根本没办法这么做了。为了解决这个问题，也就引出来下面两个研究思路。</p>
<p>（3）迁移学习Transfer Learning。既然在真实环境不行，而仿真环境可以，那么是不是可以先在仿真环境中训练好，再把知识迁移到真实机器人上。Google Deepmind在这一块做了一些不错的工作，提出了Progressive Neural Net和PathNet，验证了迁移的可能性。而且很显然的，仿真环境越真实，迁移效果会越好。那么，搞一个非常仿真的环境就非常有意义了。这不，Nvidia 刚刚推出Isaac机器人模拟系统，确实是对Robot Learning的研究注入了一剂强心剂。</p>
<p>（4）Imitation Learning 模仿学习/Few Shot Learning 少样本学习/ Meta Learning 学会学习。这是另一条思路，那就是尽量减少数据的使用量。我们如果能够教机器人几次机器人就能学会技能那么问题也能解决。而这一块也就是OpenAI (依然是Sergey Levine）那帮人在如火如荼的研究的方向。而且特别是Meta Learning，直指通用人工智能的核心。如果能够在Meta Learning上取得突破，那么本身会是革命性的。</p>
<p>因此，Robot Learning发展到这里，把研究的方向就聚焦到第三和第四点上了，并且也取得了一定的成果，但是显然还有非常多的工作可以去做。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a href="https://zh.wikipedia.org/wiki/深度学习" target="_blank" rel="noopener">深度学习</a>, by wikipedia.</li>
<li><a href="https://zhuanlan.zhihu.com/p/21262246?refer=intelligentunit" target="_blank" rel="noopener">DQN 从入门到放弃1 DQN与增强学习</a>, by Flood Sung.</li>
<li><a href="https://zhuanlan.zhihu.com/p/21292697?refer=intelligentunit" target="_blank" rel="noopener">DQN 从入门到放弃2 增强学习与MDP</a>, by Flood Sung.</li>
<li><a href="https://zhuanlan.zhihu.com/p/21340755?refer=intelligentunit" target="_blank" rel="noopener">DQN 从入门到放弃3 价值函数与Bellman方程</a>, by Flood Sung.</li>
<li><a href="https://zhuanlan.zhihu.com/p/21378532" target="_blank" rel="noopener">DQN 从入门到放弃4 动态规划与Q-Learning</a>,by Flood Sung.</li>
<li><a href="https://zhuanlan.zhihu.com/p/21421729" target="_blank" rel="noopener">DQN从入门到放弃5 深度解读DQN算法</a>, by Flood Sung.</li>
<li><a href="https://zhuanlan.zhihu.com/p/21547911" target="_blank" rel="noopener">DQN从入门到放弃6 DQN的各种改进</a>, by Flood Sung.</li>
<li><a href="https://zhuanlan.zhihu.com/p/21609472" target="_blank" rel="noopener">DQN从入门到放弃7 连续控制DQN算法-NAF</a>,by Flood Sung.</li>
<li><a href="https://zhuanlan.zhihu.com/p/26988866" target="_blank" rel="noopener">最前沿：机器人学习Robot Learning的发展</a>,by Flood Sung.</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangwang.github.io/2018/11/24/wiringPi学习笔记/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Huang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Huang's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/24/wiringPi学习笔记/" itemprop="url">
                  wiringPi学习笔记
                </a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-24 20:26:31" itemprop="dateCreated datePublished" datetime="2018-11-24T20:26:31+08:00">2018-11-24</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-12-21 21:23:41" itemprop="dateModified" datetime="2018-12-21T21:23:41+08:00">2018-12-21</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>wiringPi 库是由 Gordon Henderson 所编写并维护的一个用 C 语言写成的类库。起初,主要是作为 BCM2835 芯片的 GPIO 库。而现在,已经非常丰富,除了 GPIO 库,还包括了I2C 库、SPI 库、UART 库和软件 PWM 库等。</p>
<p>由于其与 Arduino 的“wiring”系统较为类似,故以此命名。它是采用 GNU LGPLv3许可证的,可以在 C 或 C++上使用,而且在其他编程语言上也有对应的扩展。</p>
<p>wiringPi 库包含了一个命令行工具 gpio,它可以用来设置 GPIO 管脚,可以用来读写GPIO 管脚,甚至可以在 Shell 脚本中使用来达到控制 GPIO 管脚的目的。</p>
<h1 id="下载、编译和测试wiringPi"><a href="#下载、编译和测试wiringPi" class="headerlink" title="下载、编译和测试wiringPi"></a>下载、编译和测试wiringPi</h1><ol>
<li><p>下载并编译wiringPi</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone git://git.drogon.net/wiringPi</span><br><span class="line">cd wiringPi</span><br><span class="line">./build</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试wiringPi是否安装成功<br>打开命令终端，可以通过 gpio 命令来检查 wiringPi 是否安装成功，运行下面的命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gpio –v</span><br><span class="line">gpio readall</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>运行上述第二条命令将得到树莓派的 GPIO 接口之间的对应关系。</p>
<p align="center">
    <img src="images/gpio_map.png" width="100%" alt="gpio管脚定义">
</p>

<h1 id="使用wiringPi"><a href="#使用wiringPi" class="headerlink" title="使用wiringPi"></a>使用wiringPi</h1><p>在使用wiringPi之前，应首先对wiringPi进行设置。</p>
<h2 id="wiringPi设置函数"><a href="#wiringPi设置函数" class="headerlink" title="wiringPi设置函数"></a>wiringPi设置函数</h2><p>wiringPi设置函数如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">int wiringPiSetup (void) ;</span><br><span class="line">int wiringPiSetupGpio (void) ;</span><br><span class="line">int wiringPiSetupPhys (void) ;</span><br><span class="line">int wiringPiSetupSys (void) ;</span><br></pre></td></tr></table></figure></p>
<h3 id="wiringPiSetup-void-函数"><a href="#wiringPiSetup-void-函数" class="headerlink" title="wiringPiSetup (void) 函数"></a>wiringPiSetup (void) 函数</h3><p>该函数初始化wiringPi，假定程序将使用wiringPi的管脚定义图。具体管脚映射，可以通过gpio readall命令来查看。该函数需要root权限。</p>
<h3 id="wiringPiSetupGpio-void-函数"><a href="#wiringPiSetupGpio-void-函数" class="headerlink" title="wiringPiSetupGpio (void) 函数"></a>wiringPiSetupGpio (void) 函数</h3><p>该函数与wiringPiSetup函数类似，区别在于假定程序使用的是Broadcom的GPIO管脚定义，而没有重新映射。该函数需要root权限，需要注意v1和v2版本的树莓派是不同的。</p>
<h3 id="wiringPiSetupPhys-void-函数"><a href="#wiringPiSetupPhys-void-函数" class="headerlink" title="wiringPiSetupPhys (void) 函数"></a>wiringPiSetupPhys (void) 函数</h3><p>该函数与wiringPiSetup函数类似，区别在于允许程序使用物理管脚定义，但仅支持P1接口。该函数需要root权限。</p>
<h3 id="wiringPiSetupSys-void-函数"><a href="#wiringPiSetupSys-void-函数" class="headerlink" title="wiringPiSetupSys (void) 函数"></a>wiringPiSetupSys (void) 函数</h3><p>该函数初始化wiringPi，使用/sys/class/gpio接口，而不是直接通过操作硬件来实现。该函数可以使用非root权限用户，在此种模式下的管脚号是Broadcom的GPIO管脚号，不wiringPiSetupGpio函数类似，需要注意v1和v2板子的不同。</p>
<p>在此种模式下，在运行程序前，您需要通过/sys/class/gpio接口导出要使用的管脚。你可以在一个独立的shell脚本中来导出将要使用的管脚，或者使用系统的system()函数来调用GPIO命令。</p>
<h2 id="软件-PWM-库"><a href="#软件-PWM-库" class="headerlink" title="软件 PWM 库"></a>软件 PWM 库</h2><p>wiringPi 中包含了一个软件驱动的 PWM (Pulse Width Modulation, 脉冲宽度调节)处理库，可以在任意的树莓派 GPIO 上输出 PWM 信号。</p>
<p>但是也有一些限制。为了维护较低的 CPU 使用率，最小的脉冲宽度是 100 微秒，结合默认的建议值为 100，那么最小的 PWM 频率是 100Hz。如果需要更高的频率，可以使用更低的数值。如果看脉冲宽度的驱动代码，你会发现低于 100 微秒，wiringPi 是在软件循环中实现的，这就意味着 CPU 使用率将会动态增加，从而使得控制其他管脚成为不可能。</p>
<p>需要注意的是，当其他程序运行在更高的实时的优先级，Linux 可能会影响产生信号的精度。尽管有这些限制，控制 LED 或电机还是可以的。</p>
<p>使用前,需要包含相应的文件:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;wiringPi.h&gt;</span><br><span class="line">#include &lt;softPwm.h&gt;</span><br></pre></td></tr></table></figure></p>
<p>当编译程序时,必须加上 pthread 库,如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcc –o myprog myprog.c –lwiringPi –lpthread</span><br></pre></td></tr></table></figure></p>
<p>必须使用 wiringPiSetup()、 wiringPiSetupGpio()或者 wiringPiSetupPhys()函数来初始化 wiringPi。</p>
<p>wiringPiSetupSys()是不够快的，因此，必须使用 sudo 命令来运行程序。</p>
<h3 id="softPwmCreate-函数"><a href="#softPwmCreate-函数" class="headerlink" title="softPwmCreate 函数"></a>softPwmCreate 函数</h3><p>该函数的原型为：int softPwmCreate(int pin, int initialValue, int pwmRange);</p>
<p>该函数将会创建一个软件控制的 PWM 管脚。可以使用任何一个 GPIO 管脚 , pwmRange 参数可以为 0(关)~100(全开)。</p>
<p>返回值为 0，代表成功，其他值，代表失败。</p>
<h3 id="softPwmWrite-函数"><a href="#softPwmWrite-函数" class="headerlink" title="softPwmWrite 函数"></a>softPwmWrite 函数</h3><p>该函数的原型为: void softPwmWrite(int pin, int value);</p>
<p>该函数将会更新指定管脚的 PWM 值。value 参数的范围将会被检查,如果指定的管脚之前没有通过 softPwmCreate 初始化,将会被忽略。</p>
<h2 id="wiringPi核心函数"><a href="#wiringPi核心函数" class="headerlink" title="wiringPi核心函数"></a>wiringPi核心函数</h2><h3 id="pinMode函数"><a href="#pinMode函数" class="headerlink" title="pinMode函数"></a>pinMode函数</h3><p>该函数的原型为: void pinMode(int pin, int mode);</p>
<p>使用该函数可以将某个管脚讴置为 INPUT(输入)、 OUTPUT(输出)、 PWM_OUTPUT(脉冲输出)或者 GPIO_CLOCK(GPIO 时钟)。</p>
<p>需要注意的是仅有管脚 1(BCM_GPIO 18)支持 PWM_OUTPUT 模式,仅有管脚 7(BCM_GPIO 4)支持 CLOCK 输出模式。</p>
<p>在 Sys 模式下,返个函数没有影响。你可以通过调用 gpio 命令在 shell 脚本中来设置管脚的模式。</p>
<h3 id="digitalWrite-函数"><a href="#digitalWrite-函数" class="headerlink" title="digitalWrite 函数"></a>digitalWrite 函数</h3><p>该函数的原型为: void digitalWrite(int pin, int value);</p>
<p>使用该函数可以向指定的管脚写入 HIGH(高)或者 LOW(低),写入前,需要将管脚设置为输出模式。</p>
<p>wiringPi 将任何的非 0 值作为 HIGH (高)来对待,因此, 0 是唯一能够代表 LOW (低)的数值。</p>
<h3 id="digitalRead-函数"><a href="#digitalRead-函数" class="headerlink" title="digitalRead 函数"></a>digitalRead 函数</h3><p>该函数原型： digitalRead(int pin);</p>
<p>使用该函数可以读取指定管脚的值，读取到的值为HIGH（1）或者LOW（0），该值取决于该管脚的逻辑电平的高低。</p>
<h2 id="时间函数"><a href="#时间函数" class="headerlink" title="时间函数"></a>时间函数</h2><h3 id="delay-函数"><a href="#delay-函数" class="headerlink" title="delay 函数"></a>delay 函数</h3><p>该函数的原型为：void delay(unsigned int howLong);</p>
<p>该函数将会中断程序执行至少 howLong 毫秒。因为 Linux 是多任务的原因，中断时间可能会更长。需要注意的是，最长的延迟值是一个无符号 32 位整数，其大约为 49 天。</p>
<h3 id="delayMicroseconds-函数"><a href="#delayMicroseconds-函数" class="headerlink" title="delayMicroseconds 函数"></a>delayMicroseconds 函数</h3><p>该函数的原型为：void delayMicroseconds(unsigned int howLong);</p>
<p>该函数将会中断程序执行至少 howLong 微秒。因为 Linux 是一个多任务的系统，因此中断时间可能会更长。需要注意的是，最长的延迟值是一个无符号 32 位整数，其大约为 71分钟。</p>
<p>延迟低于100 微秒，将会使用硬件循环来实现；超过 100 微秒，将会使用系统的nanosleep()函数来实现。</p>
<h2 id="优先级-时间-线程"><a href="#优先级-时间-线程" class="headerlink" title="优先级/时间/线程"></a>优先级/时间/线程</h2><h3 id="wiringPiISR-函数"><a href="#wiringPiISR-函数" class="headerlink" title="wiringPiISR 函数"></a>wiringPiISR 函数</h3><p>该函数的原型为：int wiringPiISR(int pin, int edgeType, void (*function)(void));</p>
<p>该函数会在指定管脚注册一个中断事件的函数，当指定管脚发生中断事件时，会自动调用该函数。</p>
<p>edgeType 参数可以为 INT_EDGE_FALLING（下降沿）、INT_EDGE_RISING（上升沿）、INT_EDGE_BOTH（上升沿或者下降沿）或者 INT_EDGE_SETUP。如果是INT_EDGE_SETUP，将不会初始化该管脚，因为它假定已经在别处设置过该管脚（比如使用 gpio 命令），但是，如果指定另外的类型，指定管脚将会被导出并初始化。完成此操作使用的是 gpio 命令，所以，必须保证 gpio 命令是可用的。</p>
<p>注册函数在中断触发时，将会被调用。在调用注册函数前，中断事件将会从分配器中清除，所以，即使有后续的触发发生，在处理完成前，也不会错过此次触发。（当然，如果在正在处理触发时，有不止一个的中断发生，已经发生的中断将会被忽略）。</p>
<h2 id="I2C库"><a href="#I2C库" class="headerlink" title="I2C库"></a>I2C库</h2><p>wiringPi 包含了一个 I2C 库，来让您能够更轻松的使用树莓派的板上 I2C 接口。在使用 I2C 接口之前，您可能需要使用 gpio 命令来加载 I2C 驱劢到内核中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpio load i2c</span><br></pre></td></tr></table></figure></p>
<p>如果你需要的波特率是 100Kbps，那么您可以使用如下命令设置波特率为1000Kbps：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpio load i2c 1000</span><br></pre></td></tr></table></figure></p>
<p>使用 I2C 库，需要包含 wiringPiI2C.h 文件。并且编译时，同样需要使用-lwiringPi 来连接到 wiringPi 库。</p>
<p>您仍然可以使用标准的系统命令来检测 I2C 设备，如 i2cdetect 命令，需要注意的是，在 v1 版本的树莓派上是 0，v2 版本上是 1，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ls /dev/i2c-1</span><br><span class="line"># 注意i2c-1后面的编码1</span><br><span class="line">i2cdetect –y 1</span><br></pre></td></tr></table></figure>
<p>当然，您也可以使用 gpio 命令来调用 i2cdetect 命令，从而检测 I2C 讴备，返样就不用在乎您的树莓派版本了，如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gpio i2cdetect</span><br></pre></td></tr></table></figure></p>
<h3 id="wiringPiI2CSetup-函数"><a href="#wiringPiI2CSetup-函数" class="headerlink" title="wiringPiI2CSetup 函数"></a>wiringPiI2CSetup 函数</h3><p>该函数的原型为：int wiringPiI2CSetup(int devId);</p>
<p>该函数使用指定设备标示号来初始化 I2C 系统。参数 devId 是 I2C 设备的地址，可以通过 i2cdetect 命令可以查到该地址。该函数会获取树莓派的版本并依据此打开/dev 目录下对应的讴备。</p>
<p>返回值是标准的 Linux 文件句柄，如果有错误，则返回-1。</p>
<p>比如，流行的 MCP23017 GPIO 扩展器的设备 ID 是 0x20，所以，你需要将这个数值传递给 wiringPiI2CSetup()。</p>
<h3 id="wiringPiI2CWrite-函数"><a href="#wiringPiI2CWrite-函数" class="headerlink" title="wiringPiI2CWrite 函数"></a>wiringPiI2CWrite 函数</h3><p>该函数的原型为：int wiringPiI2CWrite(int fd, int data)；</p>
<p>简单的设备写操作。一些设备可以接受数据，而不需要发送任何内部寄存器地址。</p>
<h3 id="wiringPiI2CRead-函数"><a href="#wiringPiI2CRead-函数" class="headerlink" title="wiringPiI2CRead 函数"></a>wiringPiI2CRead 函数</h3><p>该函数的原型为：int wiringPiI2CRead(int fd)；</p>
<p>简单的设备读操作。一些设备可以直接读取，而不需要发送任何寄存器地址。</p>
<h3 id="wiringPiI2CWriteReg8-和-wiringPiI2CWriteReg16-函数"><a href="#wiringPiI2CWriteReg8-和-wiringPiI2CWriteReg16-函数" class="headerlink" title="wiringPiI2CWriteReg8 和 wiringPiI2CWriteReg16 函数"></a>wiringPiI2CWriteReg8 和 wiringPiI2CWriteReg16 函数</h3><p>该函数的原型为：</p>
<p>int wiringPiI2CWriteReg8(int fd, int reg, int data); int wiringPiI2CWriteReg16(int fd, int reg, int data);</p>
<p>使用返两个函数，可以写一个 8 位或 16 位数值到指定的设备寄存器。</p>
<h3 id="wiringPiI2CReadReg8-和-wiringPiI2CReadReg16-函数"><a href="#wiringPiI2CReadReg8-和-wiringPiI2CReadReg16-函数" class="headerlink" title="wiringPiI2CReadReg8 和 wiringPiI2CReadReg16 函数"></a>wiringPiI2CReadReg8 和 wiringPiI2CReadReg16 函数</h3><p>该函数的原型为：</p>
<p>int wiringPiI2CReadReg8(int fd, int reg); int wiringPiI2CReadReg16(int fd, int reg);</p>
<p>使用返两个函数，可以从指定的设备寄存器读取一个 8 位或 16 位的数值。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a href="http://blog.lxx1.com/树莓派-wiringpi-用户手册" target="_blank" rel="noopener">树莓派 wiringPi 用户手册
</a>, by 科技爱好者博客</li>
<li><a href="http://wiringpi.com/" target="_blank" rel="noopener">Wiring Pi</a></li>
<li><a href="https://hanbingyan.github.io/2016/03/07/pthread_on_linux/" target="_blank" rel="noopener">Pthreads 入门教程</a>,by hanbingyan.</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangwang.github.io/2018/11/21/人工智能——人类科技再次飞跃的门槛/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Huang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Huang's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/21/人工智能——人类科技再次飞跃的门槛/" itemprop="url">
                  人工智能——人类科技再次飞跃的门槛
                </a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-21 10:13:10" itemprop="dateCreated datePublished" datetime="2018-11-21T10:13:10+08:00">2018-11-21</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-01-17 21:50:27" itemprop="dateModified" datetime="2019-01-17T21:50:27+08:00">2019-01-17</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>人工智能的研究历史有着一条从以“推理”为重点，到以“知识”为重点，再到以“学习”为重点的自然、清晰的脉络。机器学习则是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。</p>
<h1 id="问题的提出"><a href="#问题的提出" class="headerlink" title="问题的提出"></a>问题的提出</h1><h1 id="问题的分析"><a href="#问题的分析" class="headerlink" title="问题的分析"></a>问题的分析</h1><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol>
<li><a href="https://zh.wikipedia.org/zh-hans/机器学习" target="_blank" rel="noopener">机器学习</a>, by wikipedia.</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangwang.github.io/2018/11/20/机器学习之神经网络/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Huang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Huang's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/20/机器学习之神经网络/" itemprop="url">
                  机器学习之神经网络
                </a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-20 23:36:43" itemprop="dateCreated datePublished" datetime="2018-11-20T23:36:43+08:00">2018-11-20</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-03-17 21:54:09" itemprop="dateModified" datetime="2019-03-17T21:54:09+08:00">2019-03-17</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>人工神经网络（英语：Artificial Neural Network，ANN），简称神经网络（Neural Network，NN）或类神经网络，在机器学习和认知科学领域，是一种模仿生物神经网络（动物的中枢神经系统，特别是大脑）的结构和功能的数学模型或计算模型，用于对函数进行估计或近似。</p>
<p>神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统，通俗的讲就是具备学习功能。<strong>现代神经网络是一种非线性统计性数据建模工具</strong>。</p>
<h1 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h1><p>神经元示意图：</p>
<p></p><p align="center">
    <img src="images/Ncell.png" width="90%" alt="神经元示意图">
</p><p></p>
<center>图1 神经元示意图</center>

<ul>
<li>a1~an为输入向量的各个分量</li>
<li>w1~wn为神经元各个突触的权值</li>
<li>b为偏置</li>
<li>f为传递函数，通常为非线性函数。一般有Sigmoid(), ReLU(), Softmax()。</li>
<li>t为神经元输出</li>
</ul>
<p>神经元的数学表示是：$t=f(\vec{W^{‘}}\vec{A}+b)$</p>
<ul>
<li>$\vec{W}$为权向量，$\vec{W^{‘}}$为$\vec{W}$的转置</li>
<li>$\vec{A}$为输入向量</li>
<li>$b$为偏置</li>
<li>$f$为传递函数</li>
</ul>
<p>可见，一个神经元的功能是求得输入向量与权向量的内积后，经一个非线性传递函数得到一个标量结果。</p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><ul>
<li>Sigmoid函数</li>
</ul>
<p>Sigmoid函数也称S型激活函数，其将加权和转换为介于 0 和 1 之间的值。</p>
<script type="math/tex; mode=display">F(x)=\frac{1} {1+e^{-x}}</script><p>曲线图如下：</p>
<p></p><p align="center">
    <img src="images/sigmoid.svg" width="90%" alt="S 型激活函数">
</p><p></p>
<center>图2 S 型激活函数</center>

<ul>
<li>ReLU函数</li>
</ul>
<p>相较于 S 型函数等平滑函数，以下修正线性单元激活函数（简称为 ReLU）的效果通常要好一点，同时还非常易于计算。</p>
<script type="math/tex; mode=display">F(x)=max(0,x)</script><p>ReLU 的优势在于它基于实证发现（可能由 ReLU 驱动），拥有更实用的响应范围。S 型函数的响应性在两端相对较快地减少。</p>
<p></p><p align="center">
    <img src="images/relu.svg" width="90%" alt="ReLU 激活函数">
</p><p></p>
<center>图3 ReLU 激活函数</center>

<ul>
<li>Softmax函数</li>
</ul>
<p>Softmax函数用于多类别神经网络。在多类别问题中，Softmax 会为每个类别分配一个用小数表示的概率。这些用小数表示的概率相加之和必须是 1.0。</p>
<p>Softmax 方程式如下所示：</p>
<script type="math/tex; mode=display">p(y = j|\textbf{x})  = \frac{e^{(\textbf{w}_j^{T}\textbf{x} + b_j)}}{\sum_{k\in K} {e^{(\textbf{w}_k^{T}\textbf{x} + b_k)}} }</script><p>请注意，此公式本质上是将逻辑回归公式延伸到了多类别。</p>
<p>Softmax 层是紧挨着输出层之前的神经网络层。Softmax 层必须和输出层拥有一样的节点数。</p>
<p></p><p align="center">
    <img src="images/SoftmaxLayer.svg" width="90%" alt="神经网络中的 Softmax 层">
</p><p></p>
<center>图4 神经网络中的 Softmax 层</center>

<h1 id="神经元网络"><a href="#神经元网络" class="headerlink" title="神经元网络"></a>神经元网络</h1><p>神经元网络可分为单层神经元网络和多层神经元网络。而常用的是多层神经元网络。</p>
<h2 id="多层神经元网络"><a href="#多层神经元网络" class="headerlink" title="多层神经元网络"></a>多层神经元网络</h2><p>一种常见的多层结构的前馈网络（Multilayer Feedforward Network）由三部分组成，如图2所示：</p>
<p></p><p align="center">
    <img src="images/activation.svg" width="90%" alt="包含激活函数的三层模型的图表">
</p><p></p>
<center>图5 包含激活函数的三层模型的图表</center>

<ul>
<li>输入层（Input layer），众多神经元（Neuron）接受大量非线形输入消息。输入的消息称为输入向量。</li>
<li>输出层（Output layer），消息在神经元链接中传输、分析、权衡，形成输出结果。输出的消息称为输出向量。</li>
<li>隐藏层（Hidden layer），简称“隐层”，是输入层和输出层之间众多神经元和链接组成的各个层面。隐层可以有一层或多层。隐层的节点（神经元）数目不定，但数目越多神经网络的非线性越显著，从而神经网络的强健性（robustness）（控制系统在一定结构、大小等的参数摄动下，维持某些性能的特性）更显著。习惯上会选输入节点1.2至1.5倍的节点。</li>
</ul>
<p>这种网络一般称为感知器（对单隐藏层）或多层感知器（对多隐藏层），神经网络的类型已经演变出很多种，这种分层的结构也并不是对所有的神经网络都适用。</p>
<h2 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h2><p>通常使用反向传播算法训练神经网络<sup>[3]</sup>，但一些常见情况都会导致反向传播算法出错。</p>
<ul>
<li>梯度消失</li>
</ul>
<p>较低层（更接近输入）的梯度可能会变得非常小。在深度网络中，计算这些梯度时，可能涉及许多小项的乘积。</p>
<p>当较低层的梯度逐渐消失到 0 时，这些层的训练速度会非常缓慢，甚至不再训练。</p>
<p>ReLU 激活函数有助于防止梯度消失。</p>
<ul>
<li>梯度爆炸</li>
</ul>
<p>如果网络中的权重过大，则较低层的梯度会涉及许多大项的乘积。在这种情况下，梯度就会爆炸：梯度过大导致难以收敛。</p>
<p>批标准化可以降低学习速率，因而有助于防止梯度爆炸。</p>
<ul>
<li>ReLU 单元消失</li>
</ul>
<p>一旦 ReLU 单元的加权和低于 0，ReLU 单元就可能会停滞。它会输出对网络输出没有任何贡献的 0 激活，而梯度在反向传播算法期间将无法再从中流过。由于梯度的来源被切断，ReLU 的输入可能无法作出足够的改变来使加权和恢复到 0 以上。</p>
<p>降低学习速率有助于防止 ReLU 单元消失。</p>
<ul>
<li>丢弃正则化</li>
</ul>
<p>这是称为丢弃的另一种形式的正则化，可用于神经网络。其工作原理是，在梯度下降法的每一步中随机丢弃一些网络单元。丢弃得越多，正则化效果就越强。</p>
<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p>卷积神经⽹络（convolutional neural network）是含有卷积层（convolutional layer）的神经⽹<br>络。下面将按时间顺序介绍各类经典的卷积神经网络。</p>
<h2 id="卷积神经⽹络（LeNet）"><a href="#卷积神经⽹络（LeNet）" class="headerlink" title="卷积神经⽹络（LeNet）"></a>卷积神经⽹络（LeNet）</h2><p>LeNet是⼀个早期⽤来识别⼿写数字图像的卷积神经⽹络，于80 年代末期提出。这个名字来源于LeNet论⽂的第⼀作者Yann LeCun。LeNet展⽰了通过梯度下降训练卷积神经⽹络可以达到⼿写数字识别在当时最先进的结果。这个奠基性的⼯作第⼀次将卷积神经⽹络推上舞台，为世⼈所知。</p>
<h3 id="LeNet结构"><a href="#LeNet结构" class="headerlink" title="LeNet结构"></a>LeNet结构</h3><p>LeNet一共有7层（不包括输入层），可分为卷积层块和全连接层块两个部分，如图6所示。</p>
<p></p><p align="center">
    <img src="images/LeNet.png" width="90%" alt="LeNet结构">
</p><p></p>
<center>图6 LeNet结构</center>

<ul>
<li>输入层：输入图像的大小为32*32，这要比mnist数据库中的最大字母（28*28）还大。作用： 图像较大，这样做的目的是希望潜在的明显特征，比如笔画断续，角点等能够出现在最高层特征监测子感受野的中心。</li>
<li>卷积层：C1，C3，C5为卷积层，S2，S4为降采样层。</li>
<li>全连接层：F6为全连接层，还有一个输出层。</li>
</ul>
<h2 id="深度卷积神经⽹络（AlexNet）"><a href="#深度卷积神经⽹络（AlexNet）" class="headerlink" title="深度卷积神经⽹络（AlexNet）"></a>深度卷积神经⽹络（AlexNet）</h2><p>2012年，AlexNet横空出世。这个模型的名字来源于论⽂第⼀作者的姓名Alex Krizhevsky。AlexNet使⽤了8层卷积神经⽹络，并以很⼤的优势赢得了ImageNet 2012图像识别挑战赛。它⾸次证明了学习到的特征可以超越⼿⼯设计的特征，从而⼀举打破计算机视觉研究的前状。</p>
<h3 id="AlexNet与LeNet区别"><a href="#AlexNet与LeNet区别" class="headerlink" title="AlexNet与LeNet区别"></a>AlexNet与LeNet区别</h3><p>AlexNet与LeNet的设计理念⾮常相似，但也有显著的区别。</p>
<p>第⼀，与相对较小的LeNet相⽐，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。</p>
<p>第⼆，AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数。</p>
<p>第三，AlexNet通过丢弃法来控制全连接层的模型复杂度。</p>
<p>第四，AlexNet引⼊了⼤量的图像增⼴，如翻转、裁剪和颜⾊变化，从而进⼀步扩⼤数据集来缓解过拟合。</p>
<h3 id="AlexNet结构"><a href="#AlexNet结构" class="headerlink" title="AlexNet结构"></a>AlexNet结构</h3><p>AlexNet的一些参数和结构图： </p>
<ul>
<li>卷积层：5层 </li>
<li>全连接层：3层 </li>
<li>深度：8层 </li>
<li>参数个数：60M </li>
<li>神经元个数：650k </li>
<li>分类数目：1000类</li>
</ul>
<p></p><p align="center">
    <img src="images/AlexNet.jpg" width="90%" alt="AlexNet结构">
</p><p></p>
<center>图7 AlexNet结构</center>

<p>由于当时的显卡容量问题，AlexNet 的60M个参数无法全部放在一张显卡上操作，所以采用了两张显卡分开操作的形式，其中在C3，R1，R2，R3层上出现交互，所谓的交互就是通道的合并，是一种串接操作。</p>
<h2 id="使⽤重复元素的⽹络（VGG网络）"><a href="#使⽤重复元素的⽹络（VGG网络）" class="headerlink" title="使⽤重复元素的⽹络（VGG网络）"></a>使⽤重复元素的⽹络（VGG网络）</h2><p>VGG的名字来源于论⽂作者所在的实验室Visual Geometry Group。2014年VGG提出了可以通过重复使⽤简单的基础块来构建深度模型的思路。</p>
<p>VGG块的组成规律是：连续使⽤数个相同的填充为1、窗口形状为3*3的卷积层后接上⼀个步幅为2、窗口形状为2*2的最⼤池化层。卷积层保持输⼊的⾼和宽不变，而池化层则对其减半。</p>
<p>VGG相比AlexNet的一个改进是采用连续的几个3x3的卷积核代替AlexNet中的较大卷积核（11x11，7x7，5x5）。对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核是优于采用大的卷积核，因为多层非线性层可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。</p>
<p>与AlexNet和LeNet⼀样，VGG⽹络由卷积层模块后接全连接层模块构成。卷积层模块串联数个vgg_block，其超参数由变量conv_arch定义。该变量指定了每个VGG块⾥卷积层个数和输出通道数。全连接模块则跟AlexNet中的⼀样。</p>
<p>构造⼀个最简单的VGG⽹络VGG-11。它有5个卷积块，前2块使⽤单卷积层，而后3块使⽤双卷积层。第⼀块的输出通道是64，之后每次对输出通道数翻倍，直到变为512。</p>
<p></p><p align="center">
    <img src="images/vgg.png" width="90%" alt="VGG结构">
</p><p></p>
<center>图8 VGG结构</center>

<h2 id="⽹络中的⽹络（NiN）"><a href="#⽹络中的⽹络（NiN）" class="headerlink" title="⽹络中的⽹络（NiN）"></a>⽹络中的⽹络（NiN）</h2><p>在AlexNet问世不久，⽹络中的⽹络（NiN）提出即串联多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络。</p>
<p>卷积层的输⼊和输出通常是四维数组（样本，通道，⾼，宽），而全连接层的输⼊和输出则通常是⼆维数组（样本，特征）。如果想在全连接层后再接上卷积层，则需要将全连接层的输出变换为四维。1*1卷积层可以看成全连接层中空间维度（⾼和宽）上的每个元素相当于样本，通道相当于特征。因此， NiN使⽤1*1卷积层来替代全连接层，从而使空间信息能够⾃然传递到后⾯的层中去。</p>
<p>NiN结构（右边）与AlexNet、VGG（左边）的区别：</p>
<p></p><p align="center">
    <img src="images/NiN_VS_VGG.jpg" width="90%" alt="NiN与VGG区别">
</p><p></p>
<center>图9 NiN与VGG区别</center>

<p>NiN块是NiN中的基础块。它由⼀个卷积层加两个充当全连接层的1 * 1卷积层串联而成。其中第⼀个卷积层的超参数可以⾃⾏设置，而第⼆和第三个卷积层的超参数⼀般是固定的。</p>
<p>NiN重复使⽤由卷积层和代替全连接层的1 * 1卷积层构成的NiN块来构建深层⽹络。NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数的NiN块和全局平均池化层。</p>
<h2 id="含并⾏连结的⽹络（GoogLeNet）"><a href="#含并⾏连结的⽹络（GoogLeNet）" class="headerlink" title="含并⾏连结的⽹络（GoogLeNet）"></a>含并⾏连结的⽹络（GoogLeNet）</h2><p>在2014年的ImageNet图像识别挑战赛中，⼀个名叫GoogLeNet的⽹络结构⼤放异彩。它虽然在名字上向LeNet致敬，但在⽹络结构上已经很难看到LeNet的影⼦。GoogLeNet吸收了NiN中⽹络串联⽹络的思想，并在此基础上做了很⼤改进。</p>
<p>GoogLeNet中的基础卷积块叫作Inception块，得名于同名电影《盗梦空间》（Inception）。与NiN块相⽐，这个基础块在结构上更加复杂，如图所⽰。</p>
<p></p><p align="center">
    <img src="images/inception.svg" width="90%" alt="Inception块结构">
</p><p></p>
<center>图10 Inception块结构</center>

<p>Inception块⾥有4条并⾏的线路。前3条线路使⽤窗口⼤小分别是1 * 1、3 * 3和5 * 5的卷积层来抽取不同空间尺⼨下的信息，其中中间2个线路会对输⼊先做1 * 1卷积来减少输⼊通道数，以降低模型复杂度。第四条线路则使⽤3*3最⼤池化层，后接1*1卷积层来改变通道数。4条线路都使⽤了合适的填充来使输⼊与输出的⾼和宽⼀致。最后我们将每条线路的输出在通道维上连结，并输⼊接下来的层中去。</p>
<p>Inception块中可以⾃定义的超参数是每个层的输出通道数，以此来控制模型复杂度。</p>
<p>GoogLeNet跟VGG⼀样，在主体卷积部分中使⽤5个模块（block），每个模块之间使⽤步幅为2的3*3最⼤池化层来减小输出⾼宽。</p>
<h2 id="残差网络（ResNet）"><a href="#残差网络（ResNet）" class="headerlink" title="残差网络（ResNet）"></a>残差网络（ResNet）</h2><p>让我们先思考一个问题：对神经网络模型添加新的层，充分训练后的模型是否只可能更有效地降低训练误差？理论上，原模型解的空间只是新模型解的空间的子空间。也就是说，如果我们能将新添加的层训练成恒等映射 f(x)=x ，新模型和原模型将同样有效。由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。然而在实践中，添加过多的层后训练误差往往不降反升。即使利用批量归一化带来的数值稳定性使训练深层模型更加容易，该问题仍然存在。针对这一问题，何恺明等人提出了残差网络（ResNet）。它在2015年的ImageNet图像识别挑战赛夺魁，并深刻影响了后来的深度神经网络的设计。</p>
<p>让我们聚焦于神经网络局部。如图11所示，设输入为 x 。假设我们希望学出的理想映射为 f(x) ，从而作为图11上方激活函数的输入。左图虚线框中的部分需要直接拟合出该映射 f(x) ，而右图虚线框中的部分则需要拟合出有关恒等映射的残差映射 f(x)−x 。残差映射在实际中往往更容易优化。以本节开头提到的恒等映射作为我们希望学出的理想映射 f(x) 。我们只需将图11中右图虚线框内上方的加权运算（如仿射）的权重和偏差参数学成0，那么 f(x) 即为恒等映射。实际中，当理想映射 f(x) 极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。<strong>图11右图也是ResNet的基础块，即残差块（residual block）。在残差块中，输入可通过跨层的数据线路更快地向前传播。</strong></p>
<p></p><p align="center">
    <img src="images/residual-block.svg" width="90%" alt="ResNet残差块结构">
</p><p></p>
<center>图11 ResNet残差块结构</center>

<p>ResNet沿用了VGG全 3×3 卷积层的设计。残差块里首先有2个有相同输出通道数的 3×3 卷积层。每个卷积层后接一个批量归一化层和ReLU激活函数。然后我们将输入跳过这两个卷积运算后直接加在最后的ReLU激活函数前。这样的设计要求两个卷积层的输出与输入形状一样，从而可以相加。如果想改变通道数，就需要引入一个额外的 1×1 卷积层来将输入变换成需要的形状后再做相加运算。</p>
<h2 id="稠密连接网络（DenseNet）"><a href="#稠密连接网络（DenseNet）" class="headerlink" title="稠密连接网络（DenseNet）"></a>稠密连接网络（DenseNet）</h2><p>稠密连接网络（DenseNet）与ResNet的主要区别如图12所示。</p>
<p></p><p align="center">
    <img src="images/densenet.svg" width="90%" alt="ResNet（左）与DenseNet（右）在跨层连接上的主要区别：使用相加和使用连结">
</p><p></p>
<center>图12 ResNet（左）与DenseNet（右）在跨层连接上的主要区别：使用相加和使用连结</center>

<p>图12中将部分前后相邻的运算抽象为模块A和模块B。与ResNet的主要区别在于，DenseNet里模块B的输出不是像ResNet那样和模块A的输出相加，而是在通道维上连结。这样模块A的输出可以直接传入模块B后面的层。在这个设计里，模块A直接跟模块B后面的所有层连接在了一起。这也是它被称为“稠密连接”的原因。</p>
<p>DenseNet的主要构建模块是稠密块（dense block）和过渡层（transition layer）。前者定义了输入和输出是如何连结的，后者则用来控制通道数，使之不过大。</p>
<h2 id="MobileNets：同样的卷积层，更少的参数"><a href="#MobileNets：同样的卷积层，更少的参数" class="headerlink" title="MobileNets：同样的卷积层，更少的参数"></a>MobileNets：同样的卷积层，更少的参数</h2><p>MobileNet，正如其名，这是一个非常简单快速并且准确率也不错的CNN网络结构，它大大减少了网络层的参数数量，使得网络的前向传播和后向传播的运算量大幅减少，最终成为了一个效率极高的CNN网络。</p>
<h2 id="ShuffleNets：Group-convolution-Channel-Shuffle"><a href="#ShuffleNets：Group-convolution-Channel-Shuffle" class="headerlink" title="ShuffleNets：Group convolution+Channel Shuffle"></a>ShuffleNets：Group convolution+Channel Shuffle</h2><p>ShuffleNet是Face++提出的一种轻量化网络结构，主要思路是使用Group convolution和Channel shuffle改进ResNet，可以看作是ResNet的压缩版本。</p>
<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><p>循环神经网络是为更好地处理时序信息而设计的。它引入状态变量来存储过去的信息，并用其与当前的输入共同决定当前的输出。</p>
<p>循环神经网络常用于处理序列数据，如一段文字或声音、购物或观影的顺序，甚至是图像中的一行或一列像素。因此，循环神经网络有着极为广泛的实际应用，如语言模型、文本分类、机器翻译、语音识别、图像分析、手写识别和推荐系统。</p>
<p>现在我们考虑输入数据存在时间相关性的情况。假设 $X<em>t∈R^{n×d}$ 是序列中时间步 $t$ 的小批量输入，$H_t∈R^{n×h}$ 是该时间步的隐藏变量。与多层感知机不同的是，这里我们保存上一时间步的隐藏变量 $H</em>{t−1}$ ，并引入一个新的权重参数 $W_{hh}∈R^{h×h}$ ，该参数用来描述在当前时间步如何使用上一时间步的隐藏变量。具体来说，时间步 $t$ 的隐藏变量的计算由当前时间步的输入和上一时间步的隐藏变量共同决定：</p>
<script type="math/tex; mode=display">H_t=ϕ(X_tW_{xh}+H_{t−1}W_{hh}+b_h)</script><p>与多层感知机相比，我们在这里添加了 $H<em>{t−1}W</em>{hh}$一项。由上式中相邻时间步的隐藏变量 $H<em>t$ 和 $H</em>{t−1}$ 之间的关系可知，这里的隐藏变量能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。因此，该隐藏变量也称为隐藏状态。由于隐藏状态在当前时间步的定义使用了上一时间步的隐藏状态，上式的计算是循环的。<strong>使用循环计算的网络即循环神经网络（recurrent neural network）</strong>。</p>
<p>循环神经网络有很多种不同的构造方法。含上式所定义的隐藏状态的循环神经网络是极为常见的一种。若无特别说明，本章中的循环神经网络均基于上式中隐藏状态的循环计算。在时间步 $t$ ，输出层的输出和多层感知机中的计算类似：</p>
<script type="math/tex; mode=display">O_t=H_tW_{hq}+b_q</script><p>循环神经网络的参数包括隐藏层的权重 $W<em>{xh}∈R^{d×h}$ 、 $W</em>{hh}∈R^{h×h}$ 和偏差  $b<em>h∈R^{1×h}$ ，以及输出层的权重 $W</em>{hq}∈R^{h×q}$ 和偏差 $b_q∈R^{1×q}$ 。值得一提的是，即便在不同时间步，循环神经网络也始终使用这些模型参数。因此，循环神经网络模型参数的数量不随时间步的增加而增长。</p>
<p>图13展示了循环神经网络在3个相邻时间步的计算逻辑。在时间步 $t$ ，隐藏状态的计算可以看成是将输入 $X<em>t$ 和前一时间步隐藏状态 $H</em>{t−1}$ 连结后输入一个激活函数为 $ϕ$ 的全连接层。该全连接层的输出就是当前时间步的隐藏状态 $H<em>t$ ，且模型参数为 $W</em>{xh}$ 与 $W<em>{hh}$ 的连结，偏差为 $b_h$ 。当前时间步 $t$ 的隐藏状态 $H_t$ 将参与下一个时间步 $t+1$ 的隐藏状态 $H</em>{t+1}$ 的计算，并输入到当前时间步的全连接输出层。</p>
<p></p><p align="center">
    <img src="images/rnn.svg" width="90%" alt="含隐藏状态的循环神经网络">
</p><p></p>
<center>图13 含隐藏状态的循环神经网络</center>

<h2 id="门控循环单元（GRU）"><a href="#门控循环单元（GRU）" class="headerlink" title="门控循环单元（GRU）"></a>门控循环单元（GRU）</h2><p>当时间步数较大或者时间步较小时，循环神经网络的梯度较容易出现衰减或爆炸。虽然裁剪梯度可以应对梯度爆炸，但无法解决梯度衰减的问题。通常由于这个原因，循环神经网络在实际中较难捕捉时间序列中时间步距离较大的依赖关系。</p>
<p>门控循环神经网络（gated recurrent neural network）的提出，正是为了更好地捕捉时间序列中时间步距离较大的依赖关系。它通过可以学习的门来控制信息的流动。其中，门控循环单元（gated recurrent unit，GRU）是一种常用的门控循环神经网络。</p>
<p>门控循环单元引入了重置门（reset gate）和更新门（update gate）的概念，从而修改了循环神经网络中隐藏状态的计算方式。门控循环单元中的重置门和更新门的输入均为当前时间步输入 $X<em>t$ 与上一时间步隐藏状态 $H</em>{t−1}$ ，输出由激活函数为sigmoid函数的全连接层计算得到。</p>
<p></p><p align="center">
    <img src="images/gru_2.svg" width="90%" alt="门控循环单元中候选隐藏状态的计算">
</p><p></p>
<center>图14 门控循环单元中候选隐藏状态的计算</center>

<p>具体来说，时间步 $t$ 的候选隐藏状态 $\tilde{\boldsymbol{H}}_t∈R^{n×h}$ 的计算为</p>
<script type="math/tex; mode=display">\tilde{\boldsymbol{H}}_t=tanh(X_tW_{xh}+(R_t⊙H_{t−1})W_{hh}+b_h)</script><p>其中 $W<em>{xh}∈R^{d×h}$ 和 $W</em>{hh}∈R^{h×h}$ 是权重参数， $b_h∈R^{1×h}$ 是偏差参数。从上面这个公式可以看出，重置门控制了上一时间步的隐藏状态如何流入当前时间步的候选隐藏状态。而上一时间步的隐藏状态可能包含了时间序列截至上一时间步的全部历史信息。因此，重置门可以用来丢弃与预测无关的历史信息。</p>
<p>最后，时间步 $t$ 的隐藏状态 $H<em>t∈R^{n×h}$ 的计算使用当前时间步的更新门 $Z_t$ 来对上一时间步的隐藏状态 $H</em>{t−1}$ 和当前时间步的候选隐藏状态 $\tilde{\boldsymbol{H}}_t$ 做组合：</p>
<script type="math/tex; mode=display">Ht=Zt⊙Ht−1+(1−Zt)⊙\tilde{\boldsymbol{H}}_t</script><p></p><p align="center">
    <img src="images/gru_3.svg" width="90%" alt="门控循环单元中隐藏状态的计算">
</p><p></p>
<center>图15 门控循环单元中隐藏状态的计算</center>

<p>值得注意的是，更新门可以控制隐藏状态应该如何被包含当前时间步信息的候选隐藏状态所更新。</p>
<p>我们对门控循环单元的设计稍作总结：</p>
<ul>
<li>重置门有助于捕捉时间序列里短期的依赖关系；</li>
<li>更新门有助于捕捉时间序列里长期的依赖关系。</li>
</ul>
<h2 id="长短期记忆（LSTM）"><a href="#长短期记忆（LSTM）" class="headerlink" title="长短期记忆（LSTM）"></a>长短期记忆（LSTM）</h2><p>LSTM 中引入了3个门，即输入门（input gate）、遗忘门（forget gate）和输出门（output gate），以及与隐藏状态形状相同的记忆细胞（某些文献把记忆细胞当成一种特殊的隐藏状态），从而记录额外的信息。</p>
<h3 id="输入门、遗忘门和输出门"><a href="#输入门、遗忘门和输出门" class="headerlink" title="输入门、遗忘门和输出门"></a>输入门、遗忘门和输出门</h3><p>与门控循环单元中的重置门和更新门一样，如图16所示，长短期记忆的门的输入均为当前时间步输入 $X<em>t$ 与上一时间步隐藏状态 $H</em>{t−1}$ ，输出由激活函数为sigmoid函数的全连接层计算得到。如此一来，这3个门元素的值域均为 [0,1] 。</p>
<p></p><p align="center">
    <img src="images/lstm_0.svg" width="90%" alt="长短期记忆中输入门、遗忘门和输出门的计算">
</p><p></p>
<center>图16 长短期记忆中输入门、遗忘门和输出门的计算</center>

<p>具体来说，假设隐藏单元个数为$h$，给定时间步$t$的小批量输入$\boldsymbol{X}t \in \mathbb{R}^{n \times d}$（样本数为$n$，输入个数为$d$）和上一时间步隐藏状态$\boldsymbol{H}_{t-1} \in \mathbb{R}^{n \times h}$。 时间步$t$的输入门$\boldsymbol{I}_t \in \mathbb{R}^{n \times h}$、遗忘门$\boldsymbol{F}_t \in \mathbb{R}^{n \times h}$和输出门$\boldsymbol{O}_t \in \mathbb{R}^{n \times h}$分别计算如下：</p>
<script type="math/tex; mode=display">\begin{aligned} \boldsymbol{I}_t &= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xi} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hi} + \boldsymbol{b}_i)\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned} \boldsymbol{F}_t &= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xf} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{hf} + \boldsymbol{b}_f)\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned} \boldsymbol{O}_t &= \sigma(\boldsymbol{X}_t \boldsymbol{W}_{xo} + \boldsymbol{H}_{t-1} \boldsymbol{W}_{ho} + \boldsymbol{b}_o)\end{aligned}</script><p>其中的$\boldsymbol{W}<em>{xi}, \boldsymbol{W}</em>{xf}, \boldsymbol{W}<em>{xo} \in \mathbb{R}^{d \times h}$和$\boldsymbol{W}</em>{hi}, \boldsymbol{W}<em>{hf}, \boldsymbol{W}</em>{ho} \in \mathbb{R}^{h \times h}$是权重参数，$\boldsymbol{b}_i, \boldsymbol{b}_f, \boldsymbol{b}_o \in \mathbb{R}^{1 \times h}$是偏差参数。</p>
<h3 id="候选记忆细胞"><a href="#候选记忆细胞" class="headerlink" title="候选记忆细胞"></a>候选记忆细胞</h3><p>接下来，长短期记忆需要计算候选记忆细胞$\tilde{\boldsymbol{C}}_t$。它的计算与上面介绍的3个门类似，但使用了值域在$[-1, 1]$的tanh函数作为激活函数，如图17所示。</p>
<p></p><p align="center">
    <img src="images/lstm_1.svg" width="90%" alt="长短期记忆中候选记忆细胞的计算">
</p><p></p>
<center>图17 长短期记忆中候选记忆细胞的计算</center>

<p>具体来说，时间步$t$的候选记忆细胞$\tilde{\boldsymbol{C}}_t \in \mathbb{R}^{n \times h}$的计算为</p>
<script type="math/tex; mode=display">\tilde{\boldsymbol{C}}t = \text{tanh}(\boldsymbol{X}t \boldsymbol{W}{xc} + \boldsymbol{H}{t-1} \boldsymbol{W}_{hc} + \boldsymbol{b}_c),</script><p>其中$\boldsymbol{W}<em>{xc} \in \mathbb{R}^{d \times h}$和$\boldsymbol{W}</em>{hc} \in \mathbb{R}^{h \times h}$是权重参数，$\boldsymbol{b}_c \in \mathbb{R}^{1 \times h}$是偏差参数。</p>
<h3 id="记忆细胞"><a href="#记忆细胞" class="headerlink" title="记忆细胞"></a>记忆细胞</h3><p>我们可以通过元素值域在$[0, 1]$的输入门、遗忘门和输出门来控制隐藏状态中信息的流动，这一般也是通过使用按元素乘法（符号为$\odot$）来实现的。当前时间步记忆细胞$\boldsymbol{C}_t \in \mathbb{R}^{n \times h}$的计算组合了上一时间步记忆细胞和当前时间步候选记忆细胞的信息，并通过遗忘门和输入门来控制信息的流动：</p>
<script type="math/tex; mode=display">\boldsymbol{C}_t = \boldsymbol{F}t \odot \boldsymbol{C}{t-1} + \boldsymbol{I}_t \odot \tilde{\boldsymbol{C}}_t.</script><p>如图6.9所示，遗忘门控制上一时间步的记忆细胞$\boldsymbol{C}_{t-1}$中的信息是否传递到当前时间步，而输入门则控制当前时间步的输入$\boldsymbol{X}_t$通过候选记忆细胞$\tilde{\boldsymbol{C}}_t$如何流入当前时间步的记忆细胞。如果遗忘门一直近似1且输入门一直近似0，过去的记忆细胞将一直通过时间保存并传递至当前时间步。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。</p>
<p></p><p align="center">
    <img src="images/lstm_2.svg" width="90%" alt="长短期记忆中候选记忆细胞的计算">
</p><p></p>
<center>图18 长短期记忆中候选记忆细胞的计算</center>

<h3 id="隐藏状态"><a href="#隐藏状态" class="headerlink" title="隐藏状态"></a>隐藏状态</h3><p>有了记忆细胞以后，接下来我们还可以通过输出门来控制从记忆细胞到隐藏状态$\boldsymbol{H}_t \in \mathbb{R}^{n \times h}$的信息的流动：</p>
<script type="math/tex; mode=display">\boldsymbol{H}_t = \boldsymbol{O}_t \odot \text{tanh}(\boldsymbol{C}_t).</script><p>这里的tanh函数确保隐藏状态元素值在-1到1之间。需要注意的是，当输出门近似1时，记忆细胞信息将传递到隐藏状态供输出层使用；当输出门近似0时，记忆细胞信息只自己保留。图6.10展示了长短期记忆中隐藏状态的计算。</p>
<p></p><p align="center">
    <img src="images/lstm_3.svg" width="90%" alt="长短期记忆中隐藏状态的计算">
</p><p></p>
<center>图19 长短期记忆中隐藏状态的计算</center>

<h2 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h2><p>在深度学习应用里，我们通常会用到含有多个隐藏层的循环神经网络，也称作深度循环神经网络。图20演示了一个有 L 个隐藏层的深度循环神经网络，每个隐藏状态不断传递至当前层的下一时间步和当前时间步的下一层。</p>
<p></p><p align="center">
    <img src="images/deep-rnn.svg" width="90%" alt="深度循环神经网络的架构">
</p><p></p>
<center>图20 深度循环神经网络的架构</center>

<p>具体来说，在时间步$t$里，设小批量输入$\boldsymbol{X}_t \in \mathbb{R}^{n \times d}$（样本数为$n$，输入个数为$d$），第$\ell$隐藏层（$\ell=1,\ldots,L$）的隐藏状态为$\boldsymbol{H}_t^{(\ell)} \in \mathbb{R}^{n \times h}$（隐藏单元个数为$h$），输出层变量为$\boldsymbol{O}_t \in \mathbb{R}^{n \times q}$（输出个数为$q$），且隐藏层的激活函数为$\phi$。第1隐藏层的隐藏状态和之前的计算一样：</p>
<script type="math/tex; mode=display">\boldsymbol{H}_t^{(1)} = \phi(\boldsymbol{X}_t \boldsymbol{W}_{xh}^{(1)} + \boldsymbol{H}_{t-1}^{(1)} \boldsymbol{W}_{hh}^{(1)} + \boldsymbol{b}_h^{(1)}),</script><p>其中权重$\boldsymbol{W}<em>{xh}^{(1)} \in \mathbb{R}^{d \times h}$、$\boldsymbol{W}</em>{hh}^{(1)} \in \mathbb{R}^{h \times h}$和偏差 $\boldsymbol{b}_h^{(1)} \in \mathbb{R}^{1 \times h}$分别为第1隐藏层的模型参数。</p>
<p>当$1 &lt; \ell \leq L$时，第$\ell$隐藏层的隐藏状态的表达式为</p>
<script type="math/tex; mode=display">\boldsymbol{H}_t^{(\ell)} = \phi(\boldsymbol{H}_t^{(\ell-1)} \boldsymbol{W}_{xh}^{(\ell)} + \boldsymbol{H}_{t-1}^{(\ell)} \boldsymbol{W}_{hh}^{(\ell)} + \boldsymbol{b}_h^{(\ell)}),</script><p>其中权重$\boldsymbol{W}<em>{xh}^{(\ell)} \in \mathbb{R}^{h \times h}$、$\boldsymbol{W}</em>{hh}^{(\ell)} \in \mathbb{R}^{h \times h}$和偏差 $\boldsymbol{b}_h^{(\ell)} \in \mathbb{R}^{1 \times h}$分别为第$\ell$隐藏层的模型参数。</p>
<p>最终，输出层的输出只需基于第$L$隐藏层的隐藏状态：</p>
<script type="math/tex; mode=display">\boldsymbol{O}_t = \boldsymbol{H}_t^{(L)} \boldsymbol{W}_{hq} + \boldsymbol{b}_q,</script><p>其中权重$\boldsymbol{W}_{hq} \in \mathbb{R}^{h \times q}$和偏差$\boldsymbol{b}_q \in \mathbb{R}^{1 \times q}$为输出层的模型参数。</p>
<p>同多层感知机一样，隐藏层个数$L$和隐藏单元个数$h$都是超参数。此外，如果将隐藏状态的计算换成门控循环单元或者长短期记忆的计算，我们可以得到深度门控循环神经网络。</p>
<h2 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h2><p>之前介绍的循环神经网络模型都是假设当前时间步是由前面的较早时间步的序列决定的，因此它们都将信息通过隐藏状态从前往后传递。有时候，当前时间步也可能由后面时间步决定。例如，当我们写下一个句子时，可能会根据句子后面的词来修改句子前面的用词。双向循环神经网络通过增加从后往前传递信息的隐藏层来更灵活地处理这类信息。图21演示了一个含单隐藏层的双向循环神经网络的架构。</p>
<p></p><p align="center">
    <img src="images/birnn.svg" width="90%" alt="双向循环神经网络的架构">
</p><p></p>
<center>图21 双向循环神经网络的架构</center>

<h1 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h1><p>在概率统计理论中, 生成模型是指能够随机生成观测数据的模型，尤其是在给定某些隐含参数的条件下。它给观测值和标注数据序列指定一个联合概率分布。在机器学习中，生成模型可以用来直接对数据建模（例如根据某个变量的概率密度函数进行数据采样），也可以用来建立变量间的条件概率分布。条件概率分布可以由生成模型根据贝叶斯定理形成。</p>
<p>香农 (1948) 给出了有一个英语双词频率表生成句子的例子。可以生成如“representing and speedily is an good”这种句子。一开始并不能生成正确的英文句子，但随着词频表由双词扩大为三词甚至多词，生成的句子也就慢慢的成型了。</p>
<p>生成模型的定义与判别模型相对应：生成模型是所有变量的全概率模型，而判别模型是在给定观测变量值前提下目标变量条件概率模型。因此生成模型能够用于模拟（即生成）模型中任意变量的分布情况，而判别模型只能根据观测变量得到目标变量的采样。判别模型不对观测变量的分布建模，因此它不能够表达观测变量与目标变量之间更复杂的关系。因此，生成模型更适用于无监督的任务，如分类和聚类。</p>
<h2 id="生成对抗网络"><a href="#生成对抗网络" class="headerlink" title="生成对抗网络"></a>生成对抗网络</h2><p>生成对抗网络（英语：Generative Adversarial Network，简称GAN）是非监督式学习的一种方法，通过让两个神经网络相互博弈的方式进行学习。该方法由伊恩·古德费洛等人于2014年提出。[1]</p>
<p>生成对抗网络由一个生成网络与一个判别网络组成。生成网络从潜在空间（latent space）中随机采样作为输入，其输出结果需要尽量模仿训练集中的真实样本。判别网络的输入则为真实样本或生成网络的输出，其目的是将生成网络的输出从真实样本中尽可能分辨出来。而生成网络则要尽可能地欺骗判别网络。两个网络相互对抗、不断调整参数，最终目的是使判别网络无法判断生成网络的输出结果是否真实。[2][1][3]</p>
<p>生成对抗网络常用于生成以假乱真的图片。[4]此外，该方法还被用于生成视频[5]、三维物体模型[6]等。</p>
<h1 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h1><p>强化学习（英语：Reinforcement learning，简称RL）是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。其灵感来源于心理学中的行为主义理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。这个方法具有普适性，因此在其他许多领域都有研究，例如博弈论、控制论、运筹学、信息论、仿真优化、多主体系统学习、群体智能、统计学以及遗传算法。在运筹学和控制理论研究的语境下，强化学习被称作“近似动态规划”（approximate dynamic programming，ADP）。在最优控制理论中也有研究这个问题，虽然大部分的研究是关于最优解的存在和特性，并非是学习或者近似方面。在经济学和博弈论中，强化学习被用来解释在有限理性的条件下如何出现平衡。</p>
<p>在机器学习问题中，环境通常被规范为马可夫决策过程（MDP），所以许多强化学习算法在这种情况下使用动态规划技巧。传统的技术和强化学习算法的主要区别是，后者不需要关于MDP的知识，而且针对无法找到确切方法的大规模MDP。</p>
<p>强化学习和标准的监督式学习之间的区别在于，它并不需要出现正确的输入/输出对，也不需要精确校正次优化的行为。强化学习更加专注于在线规划，需要在探索（在未知的领域）和遵从（现有知识）之间找到平衡。强化学习中的“探索-遵从”的交换，在多臂老虎机（英语：multi-armed bandit）问题和有限MDP中研究得最多。</p>
<h2 id="理解强化学习"><a href="#理解强化学习" class="headerlink" title="理解强化学习"></a>理解强化学习</h2><p>抛开强化学习探索反馈过程，从回合的最终结果看，强化学习也是一种有监督学习。回合最终结果的输赢就是标签，如果最终结果是好的，说明之前的一系列状态动作的决策过程是有效的，反之是无效的。通过不断地学习，最终可得到较优的状态到动作地策略分布Q函数或者状态和动作的值函数。</p>
<h1 id="记忆网络"><a href="#记忆网络" class="headerlink" title="记忆网络"></a>记忆网络</h1><p>传统的深度学习模型（RNN、LSTM、GRU等）使用hidden states或者Attention机制作为他们的记忆功能，但是这种方法产生的记忆太小了，无法精确记录一段话中所表达的全部内容，也就是在将输入编码成dense vectors的时候丢失了很多信息。记忆网络采用一种可读写的外部记忆模块，并将其和inference组件联合训练，最终得到一个可以被灵活操作的记忆模块。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a href="https://zh.wikipedia.org/wiki/人工神经网络" target="_blank" rel="noopener">人工神经网络</a>,by wikipedia.</li>
<li><a href="https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/anatomy?hl=zh-cn" target="_blank" rel="noopener">神经网络简介</a>,by google.</li>
<li><a href="https://www.leiphone.com/news/201705/TMsNCqjpOIfN3Bjr.html" target="_blank" rel="noopener">一文详解神经网络 BP 算法原理及 Python 实现</a>,by AI研习社.</li>
<li><a href="https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/?hl=zh-CN" target="_blank" rel="noopener">反向传播算法动态演示</a>, by google.</li>
<li><a href="https://coggle.it/diagram/Wf5mYoJbsgABUF9P/t/neural-net-arch-genealogy" target="_blank" rel="noopener">深度学习架构家谱</a>,by hunkim.</li>
<li><a href="http://zh.d2l.ai/" target="_blank" rel="noopener">动手学深度学习</a>,by d2l-zh.</li>
<li><a href="https://blog.csdn.net/Genius_zz/article/details/52804585" target="_blank" rel="noopener">神经网络之LeNet结构分析及参数详解</a>,by Genius_zz.</li>
<li><a href="https://zhuanlan.zhihu.com/p/47391705" target="_blank" rel="noopener">经典CNN结构简析：AlexNet、VGG、NIN、GoogLeNet、ResNet etc. </a>,by Uno Whoiam.</li>
<li><a href="https://zhuanlan.zhihu.com/p/41423739" target="_blank" rel="noopener">一文读懂VGG网络</a>,by Amusi.</li>
<li><a href="https://zh.wikipedia.org/wiki/生成模型" target="_blank" rel="noopener">生成模型</a>,by wikipedia.</li>
<li><a href="https://zh.wikipedia.org/wiki/生成对抗网络" target="_blank" rel="noopener">生成对抗网络</a>,by wikipedia.</li>
<li><a href="https://poloclub.github.io/ganlab/" target="_blank" rel="noopener">GAN动态演示</a>,by poloclub.</li>
<li><a href="https://zh.wikipedia.org/wiki/强化学习" target="_blank" rel="noopener">强化学习</a>,by wikipedia.</li>
<li><a href="https://blog.csdn.net/Nicholas_Liu2017/article/details/73694666" target="_blank" rel="noopener">25张图让你读懂神经网络架构</a>, by Nicholas_Liu2017.</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangwang.github.io/2018/11/19/机器学习之相关概念/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Huang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Huang's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/19/机器学习之相关概念/" itemprop="url">
                  机器学习之相关概念
                </a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-19 16:04:16" itemprop="dateCreated datePublished" datetime="2018-11-19T16:04:16+08:00">2018-11-19</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-02-10 23:56:05" itemprop="dateModified" datetime="2019-02-10T23:56:05+08:00">2019-02-10</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>机器学习是人工智能的一个分支。人工智能的研究历史有着一条从以“推理”为重点，到以“知识”为重点，再到以“学习”为重点的自然、清晰的脉络。显然，机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能中的问题。</p>
<h1 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h1><p>机器学习有下面几种定义：</p>
<ul>
<li>机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能。</li>
<li>机器学习是对能通过经验自动改进的计算机算法的研究。</li>
<li>机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。</li>
</ul>
<p>一种经常引用的英文定义是：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</p>
<h1 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h1><p>机器学习可以分成下面几种类别：</p>
<ul>
<li>监督学习：从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求是包括输入和输出，也可以说是特征和目标。训练集中的目标是由人标注的。常见的监督学习算法包括回归分析和统计分类。</li>
</ul>
<p>监督学习和非监督学习的差别就是训练集目标是否人标注。他们都有训练集且都有输入和输出。</p>
<ul>
<li>无监督学习：与监督学习相比，训练集没有人为标注的结果。常见的无监督学习算法有生成对抗网络（GAN）、聚类。</li>
<li>半监督学习：介于监督学习与无监督学习之间。</li>
<li>强化学习：通过观察来学习做成如何的动作。每个动作都会对环境有所影响，学习对象根据观察到的周围环境的反馈来做出判断。</li>
</ul>
<h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><p>具体的机器学习算法有：</p>
<ul>
<li>构造间隔理论分布：聚类分析和模式识别<ul>
<li>人工神经网络</li>
<li>决策树</li>
<li>感知器</li>
<li>支持向量机</li>
<li>集成学习AdaBoost</li>
<li>降维与度量学习</li>
<li>聚类</li>
<li>贝叶斯分类器</li>
</ul>
</li>
<li>构造条件概率：回归分析和统计分类<ul>
<li>高斯过程回归</li>
<li>线性判别分析</li>
<li>最近邻居法</li>
<li>径向基函数核</li>
</ul>
</li>
<li>通过再生模型构造概率密度函数：<ul>
<li>最大期望算法</li>
<li>概率图模型：包括贝叶斯网和Markov随机场</li>
<li>Generative Topographic Mapping</li>
</ul>
</li>
<li>近似推断技术：<ul>
<li>马尔可夫链</li>
<li>蒙特卡罗方法</li>
<li>变分法</li>
</ul>
</li>
<li>最优化：大多数以上方法，直接或者间接使用最优化算法。</li>
</ul>
<h1 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a>机器学习基础</h1><h2 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h2><p>标签是我们要预测的事物，即简单线性回归中的 y 变量。标签可以是小麦未来的价格、图片中显示的动物品种、音频剪辑的含义或任何事物。</p>
<h2 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h2><p>特征是输入变量，即简单线性回归中的 x 变量。简单的机器学习项目可能会使用单个特征，而比较复杂的机器学习项目可能会使用数百万个特征，按如下方式指定：</p>
<script type="math/tex; mode=display">\{ x_1,x_2,...x_N \}</script><p>在垃圾邮件检测器示例中，特征可能包括：</p>
<ul>
<li>电子邮件文本中的字词</li>
<li>发件人的地址</li>
<li>发送电子邮件的时段</li>
<li>电子邮件中包含“一种奇怪的把戏”这样的短语。</li>
</ul>
<h2 id="样本"><a href="#样本" class="headerlink" title="样本"></a>样本</h2><p>样本是指数据的特定实例：x。（我们采用粗体 x 表示它是一个矢量。）我们将样本分为以下两类：</p>
<ul>
<li>有标签样本</li>
<li>无标签样本</li>
</ul>
<p>有标签样本同时包含特征和标签，常用于训练模型。。即：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">labeled examples: &#123;features, label&#125;: (x, y)</span><br></pre></td></tr></table></figure></p>
<p>无标签样本包含特征，但不包含标签，常用于模型预测。即：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unlabeled examples: &#123;features, ?&#125;: (x, ?)</span><br></pre></td></tr></table></figure></p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>模型定义了特征与标签之间的关系。例如，垃圾邮件检测模型可能会将某些特征与“垃圾邮件”紧密联系起来。我们来重点介绍一下模型生命周期的两个阶段：</p>
<ul>
<li><p>训练表示创建或学习模型。向模型展示有标签样本，让模型逐渐学习特征与标签之间的关系。</p>
</li>
<li><p>推断表示将训练后的模型应用于无标签样本。使用训练后的模型来做出有用的预测 (y’)。</p>
</li>
</ul>
<h2 id="回归与分类"><a href="#回归与分类" class="headerlink" title="回归与分类"></a>回归与分类</h2><p>回归模型可预测连续值。例如，回归模型做出的预测可回答如下问题：</p>
<ul>
<li><p>加利福尼亚州一栋房产的价值是多少？</p>
</li>
<li><p>用户点击此广告的概率是多少？</p>
</li>
</ul>
<p>分类模型可预测离散值。例如，分类模型做出的预测可回答如下问题：</p>
<ul>
<li><p>某个指定电子邮件是垃圾邮件还是非垃圾邮件？</p>
</li>
<li><p>这是一张狗、猫还是仓鼠图片？</p>
</li>
</ul>
<h2 id="损失"><a href="#损失" class="headerlink" title="损失"></a>损失</h2><p>训练模型表示通过有标签样本来学习（确定）所有权重和偏差的理想值。在监督式学习中，机器学习算法通过以下方式构建模型：检查多个样本并尝试找出可最大限度地减少损失的模型；这一过程称为经验风险最小化。</p>
<p>损失是对糟糕预测的惩罚。也就是说，损失是一个数值，表示对于单个样本而言模型预测的准确程度。如果模型的预测完全准确，则损失为零，否则损失会较大。训练模型的目标是从所有样本中找到一组平均损失“较小”的权重和偏差。</p>
<p>平方损失：又称为 $L_2$ 损失,一种常见的损失函数。例如单个样本的平方损失如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">= the square of the difference between the label and the prediction</span><br><span class="line">= (observation - prediction(x))^2</span><br><span class="line">= (y - y&apos;)^2</span><br></pre></td></tr></table></figure></p>
<p>均方误差 (MSE) 指的是每个样本的平均平方损失。要计算 MSE，请求出各个样本的所有平方损失之和，然后除以样本数量：</p>
<script type="math/tex; mode=display">
MSE = \frac{1}{N} \sum_{(x,y)\in D} (y - prediction(x))^2</script><p>其中：</p>
<ul>
<li>(x,y)指的是样本，其中<ul>
<li>x指的是模型进行预测时使用的特征集（例如，温度、年龄和交配成功率）。</li>
<li>y指的是样本的标签（例如，每分钟的鸣叫次数）。</li>
</ul>
</li>
<li>prediction(x)指的是权重和偏差与特征集 x 结合的函数。</li>
<li>D指的是包含多个有标签样本（即 (x,y)）的数据集。</li>
<li>N指的是D中的样本数量。</li>
</ul>
<h2 id="迭代方法"><a href="#迭代方法" class="headerlink" title="迭代方法"></a>迭代方法</h2><p>下图显示了机器学习算法用于训练模型的迭代试错过程：</p>
<p></p><p align="center">
    <img src="images/GradientDescentDiagram.svg" width="90%" alt="用于训练模型的迭代方法">
</p><p></p>
<center>图1 用于训练模型的迭代方法</center>

<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>计算参数更新的目标是在模型的迭代试错过程中，使损失越来越小。而常用的方法就是梯度下降法。</p>
<p></p><p align="center">
    <img src="images/convex.svg" width="90%" alt="回归问题产生的损失与权重图为凸形">
</p><p></p>
<center>图2 回归问题产生的损失与权重图为凸形</center>

<p>对于图2所示的凸形问题，刚好存在一个斜率正好为 0 的位置，即是损失函数的收敛之处。梯度下降法的第一个阶段是为$w_1$ 选择一个起始值（起点）。</p>
<p>然后，梯度下降法算法会计算损失曲线在起点处的梯度。简而言之，梯度是偏导数的矢量；它可以让您了解哪个方向距离目标“更近”或“更远”。</p>
<p>请注意，梯度是一个矢量，因此具有以下两个特征：</p>
<ul>
<li>方向</li>
<li>大小</li>
</ul>
<p>梯度始终指向损失函数中增长最为迅猛的方向。梯度下降法算法会沿着负梯度的方向走一步，以便尽快降低损失。</p>
<p>为了确定损失函数曲线上的下一个点，梯度下降法算法会将梯度大小的一部分与起点相加，如图3所示：</p>
<p></p><p align="center">
    <img src="images/GradientDescentGradientStep.svg" width="90%" alt="一个梯度步长将我们移动到损失曲线上的下一个点">
</p><p></p>
<center>图3 一个梯度步长将我们移动到损失曲线上的下一个点</center>

<p>然后，梯度下降法会重复此过程，逐渐接近最低点。</p>
<h3 id="学习速率"><a href="#学习速率" class="headerlink" title="学习速率"></a>学习速率</h3><p>梯度下降法算法用梯度乘以一个称为<strong>学习速率</strong>（有时也称为步长）的标量，以确定下一个点的位置。例如，如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。</p>
<p><strong>超参数</strong>是编程人员在机器学习算法中用于调整的旋钮。大多数机器学习编程人员会花费相当多的时间来调整学习速率。如果您选择的学习速率过小，就会花费太长的学习时间。如果您指定的学习速率过大，下一个点将永远在 U 形曲线的底部随意弹跳。</p>
<h3 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h3><p>在梯度下降法中，<strong>批量</strong>指的是用于在单次迭代中计算梯度的样本总数。到目前为止，我们一直假定批量是指整个数据集。就 Google 的规模而言，数据集通常包含数十亿甚至数千亿个样本。此外，Google 数据集通常包含海量特征。因此，一个批量可能相当巨大。如果是超大批量，则单次迭代就可能要花费很长时间进行计算。</p>
<p>通过从我们的数据集中随机选择样本，我们可以通过小得多的数据集估算（尽管过程非常杂乱）出较大的平均值。 随机梯度下降法 (SGD) 将这种想法运用到极致，它每次迭代只使用一个样本（批量大小为 1）。如果进行足够的迭代，SGD 也可以发挥作用，但过程会非常杂乱。“随机”这一术语表示构成各个批量的一个样本都是随机选择的。</p>
<p>小批量随机梯度下降法（小批量 SGD）是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。</p>
<h2 id="泛化-Generalization"><a href="#泛化-Generalization" class="headerlink" title="泛化(Generalization)"></a>泛化(Generalization)</h2><p>泛化是指机器学习对从真实概率分布（已隐藏）中抽取的新数据做出良好预测的能力。要取得良好的泛化能力，机器学习必须满足以下基本假设，同时防止过拟合。</p>
<ul>
<li><p>机器学习的基本假设：</p>
<ul>
<li>从分布中随机抽取独立同分布 (i.i.d) 的样本。换言之，样本之间不会互相影响。（另一种解释：i.i.d. 是表示变量随机性的一种方式）。</li>
<li>分布是平稳的；即分布在数据集内不会发生变化。</li>
<li>从同一分布的数据划分中抽取样本。</li>
</ul>
</li>
<li><p><strong>过拟合</strong>模型在训练过程中产生的损失很低，但在预测新数据方面的表现却非常糟糕。</p>
</li>
</ul>
<h2 id="训练集与测试集"><a href="#训练集与测试集" class="headerlink" title="训练集与测试集"></a>训练集与测试集</h2><p>机器学习模型旨在根据以前未见过的新数据做出良好预测。但是，如果您要根据数据集构建模型，如何获得以前未见过的数据呢？一种方法是将您的数据集分成两个子集：</p>
<ul>
<li>训练集 - 用于训练模型的子集。</li>
<li>测试集 - 用于测试模型的子集。</li>
</ul>
<p>测试集应满足以下两个条件：</p>
<ul>
<li>规模足够大，可产生具有统计意义的结果。</li>
<li>能代表整个数据集。换言之，挑选的测试集的特征应该与训练集的特征相同。</li>
</ul>
<h2 id="验证集"><a href="#验证集" class="headerlink" title="验证集"></a>验证集</h2><p>将数据集划分为训练集和测试集两个子集是个不错的想法，但不是万能良方。通过将数据集划分为训练集、验证集、测试集三个子集，可以大幅降低过拟合的发生几率。</p>
<p>使用验证集评估训练集的效果。然后，在模型“通过”验证集之后，使用测试集再次检查评估结果。图4展示了这一新工作流程：</p>
<p></p><p align="center">
    <img src="images/WorkflowWithValidationSet.svg" width="90%" alt="使用验证集的工作流程">
</p><p></p>
<center>图4 使用验证集的工作流程</center>

<h2 id="特征表示"><a href="#特征表示" class="headerlink" title="特征表示"></a>特征表示</h2><p>特征工程指的是将原始数据转换为特征矢量。进行特征工程预计需要大量时间。</p>
<ul>
<li>映射数值: 机器学习模型根据浮点值进行训练，因此整数和浮点原始数据不需要特殊编码。</li>
<li>映射字符串值: 首先为要表示的所有特征的字符串值定义一个词汇表。然后使用该词汇表创建一个独热编码，用于将指定字符串值表示为二元矢量。</li>
<li>映射分类（枚举）值</li>
</ul>
<p></p><p align="center">
    <img src="images/OneHotEncoding.svg" width="90%" alt="通过独热编码映射字符串值">
</p><p></p>
<center>图5 通过独热编码映射字符串值</center>

<h3 id="良好特征的特点"><a href="#良好特征的特点" class="headerlink" title="良好特征的特点"></a>良好特征的特点</h3><ul>
<li>避免很少使用的离散特征值。</li>
</ul>
<p>良好的特征值应该在数据集中出现大约 5 次以上。这样一来，模型就可以学习该特征值与标签是如何关联的。</p>
<ul>
<li>最好具有清晰明确的含义。</li>
</ul>
<p>每个特征对于项目中的任何人来说都应该具有清晰明确的含义。例如，下面的房龄适合作为特征，可立即识别为年龄：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">house_age: 27</span><br></pre></td></tr></table></figure></p>
<ul>
<li>不要将“神奇”的值与实际数据混为一谈</li>
</ul>
<p>良好的浮点特征不包含超出范围的异常断点或“神奇”的值。例如，假设一个特征具有 0 到 1 之间的浮点值。那么，如下值是可以接受的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">quality_rating: 0.82</span><br><span class="line">quality_rating: 0.37</span><br></pre></td></tr></table></figure></p>
<p>不过，如果用户没有输入 quality_rating，则数据集可能使用如下神奇值来表示不存在该值：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">quality_rating: -1</span><br></pre></td></tr></table></figure></p>
<p>为解决神奇值的问题，需将该特征转换为两个特征：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">一个特征只存储质量评分，不含神奇值。</span><br><span class="line"></span><br><span class="line">一个特征存储布尔值，表示是否提供了 quality_rating。为该布尔值特征指定一个名称，例如 is_quality_rating_defined。</span><br></pre></td></tr></table></figure></p>
<ul>
<li>考虑上游不稳定性</li>
</ul>
<p>特征的定义不应随时间发生变化。例如，下列值是有用的，因为城市名称一般不会改变。（注意，我们仍然需要将“br/sao_paulo”这样的字符串转换为独热矢量。）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">city_id: &quot;br/sao_paulo&quot;</span><br></pre></td></tr></table></figure></p>
<p>但收集由其他模型推理的值会产生额外成本。可能值“219”目前代表圣保罗，但这种表示在未来运行其他模型时可能轻易发生变化：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inferred_city_cluster: &quot;219&quot;</span><br></pre></td></tr></table></figure></p>
<h3 id="清理数据"><a href="#清理数据" class="headerlink" title="清理数据"></a>清理数据</h3><p>即使是非常少量的坏样本会破坏掉一个大规模数据集，因此需花费大量的时间挑出坏样本并加工可以挽救的样本。</p>
<ol>
<li><p>缩放特征值: 缩放是指将浮点特征值从自然范围（例如 100 到 900）转换为标准范围（例如 0 到 1 或 -1 到 +1）。如果特征集包含多个特征，则缩放特征可以带来以下优势：</p>
<ul>
<li>帮助梯度下降法更快速地收敛。</li>
<li>帮助避免“NaN 陷阱”。</li>
<li>帮助模型为每个特征确定合适的权重。</li>
</ul>
</li>
<li><p>处理极端离群值</p>
<ul>
<li>对每个值取对数</li>
<li>将最大值“限制”为某个任意值</li>
</ul>
</li>
<li>分箱</li>
<li>清查</li>
</ol>
<p>数据集中的很多样本是不可靠的，原因有以下一种或多种：</p>
<ul>
<li>遗漏值。 例如，有人忘记为某个房屋的年龄输入值。</li>
<li>重复样本。 例如，服务器错误地将同一条记录上传了两次。</li>
<li>不良标签。 例如，有人错误地将一颗橡树的图片标记为枫树。</li>
<li>不良特征值。 例如，有人输入了多余的位数，或者温度计被遗落在太阳底下。</li>
</ul>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>图6泛化曲线显示的是训练集和验证集相对于训练迭代次数的损失。</p>
<p></p><p align="center">
    <img src="images/RegularizationTwoLossFunctions.svg" width="90%" alt="训练集和验证集损失">
</p><p></p>
<center>图6 训练集和验证集损失</center>

<p>图6显示的是某个模型的训练损失逐渐减少，但验证损失最终增加。换言之，该泛化曲线显示该模型与训练集中的数据过拟合。根据奥卡姆剃刀定律，或许我们可以通过降低复杂模型的复杂度来防止过拟合，这种原则称为正则化。</p>
<p>正则化以最小化损失和复杂度为目标，这称为结构风险最小化：</p>
<script type="math/tex; mode=display">\text{minimize(Loss(Data|Model) + complexity(Model))}</script><p>现在，训练优化算法是一个由两项内容组成的函数：一个是损失项，用于衡量模型与数据的拟合度，另一个是正则化项，用于衡量模型复杂度。</p>
<p>有两种常用衡量模型复杂度的方法：</p>
<ul>
<li>将模型复杂度作为模型中所有特征的权重的函数。</li>
<li>将模型复杂度作为具有非零权重的特征总数的函数。</li>
</ul>
<p>如果模型复杂度是权重的函数，则特征权重的绝对值越高，对模型复杂度的贡献就越大。</p>
<h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p>可以使用 L2 正则化公式来量化复杂度，该公式将正则化项定义为所有特征权重的平方和：</p>
<script type="math/tex; mode=display">L_2\text{ regularization term} = ||\boldsymbol w||_2^2 = {w_1^2 + w_2^2 + ... + w_n^2}</script><p>在这个公式中，接近于 0 的权重对模型复杂度几乎没有影响，而离群值权重则可能会产生巨大的影响。</p>
<p>模型开发者通过以下方式来调整正则化项的整体影响：用正则化项的值乘以名为 lambda（又称为正则化率）的标量。也就是说，模型开发者会执行以下运算：</p>
<script type="math/tex; mode=display">\text{minimize(Loss(Data|Model)} + \lambda \text{ complexity(Model))}</script><p>执行 L2 正则化对模型具有以下影响:</p>
<ul>
<li>使权重值接近于 0（但并非正好为 0）</li>
<li>使权重的平均值接近于 0，且呈正态（钟形曲线或高斯曲线）分布。</li>
</ul>
<p>在选择 lambda 值时，目标是在简单化和训练数据拟合之间达到适当的平衡：</p>
<ul>
<li><p>如果您的 lambda 值过高，则模型会非常简单，但是您将面临数据欠拟合的风险。您的模型将无法从训练数据中获得足够的信息来做出有用的预测。</p>
</li>
<li><p>如果您的 lambda 值过低，则模型会比较复杂，并且您将面临数据过拟合的风险。您的模型将因获得过多训练数据特点方面的信息而无法泛化到新数据。</p>
</li>
</ul>
<h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><p>稀疏矢量通常包含许多维度。创建特征组合会导致包含更多维度。由于使用此类高维度特征矢量，因此模型可能会非常庞大，并且需要大量的 RAM。</p>
<p>在高维度稀疏矢量中，最好尽可能使权重正好降至 0。正好为 0 的权重基本上会使相应特征从模型中移除。 将特征设为 0 可节省 RAM 空间，且可以减少模型中的噪点。</p>
<p>L1 正则化使模型中很多信息缺乏的系数正好为 0，从而在推理时节省 RAM，同时具有凸优化的优势，可有效进行计算。</p>
<p>L2 和 L1 采用不同的方式降低权重：</p>
<ul>
<li>L2 会降低权重<sup>2</sup>。</li>
<li>L1 会降低 |权重|。</li>
</ul>
<p>因此，L2 和 L1 具有不同的导数：</p>
<ul>
<li>L2 的导数为 2 * 权重。</li>
<li>L1 的导数为 k（一个常数，其值与权重无关）。</li>
</ul>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>许多问题需要将概率估算值作为输出。逻辑回归是一种极其高效的概率计算机制。实际上，您可以通过下两种方式之一使用返回的概率：</p>
<ul>
<li>“按原样”</li>
<li>转换成二元类别。</li>
</ul>
<p>在很多情况下，您会将逻辑回归输出映射到二元分类问题的解决方案，该二元分类问题的目标是正确预测两个可能的标签（例如，“垃圾邮件”或“非垃圾邮件”）中的一个。</p>
<p>您可能想知道逻辑回归模型如何确保输出值始终落在 0 和 1 之间。巧合的是，S 型函数生成的输出值正好具有这些特性，其定义如下：</p>
<script type="math/tex; mode=display">y = \frac{1}{1 + e^{-z}}</script><p>S 型函数会产生以下曲线图：</p>
<p></p><p align="center">
    <img src="images/SigmoidFunction.png" width="90%" alt="S 型函数">
</p><p></p>
<center>图7 S 型函数</center>

<p>如果 z 表示使用逻辑回归训练的模型的线性层的输出，则 S 型(z) 函数会生成一个介于 0 和 1 之间的值（概率）。用数学方法表示为：</p>
<script type="math/tex; mode=display">y' = \frac{1}{1 + e^{-(z)}}</script><p>其中：</p>
<ul>
<li>y’ 是逻辑回归模型针对特定样本的输出。</li>
<li>z 是 b + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + … w<sub>N</sub>x<sub>N</sub><ul>
<li>“w”值是该模型学习的权重和偏差。</li>
<li>“x”值是特定样本的特征值。</li>
</ul>
</li>
</ul>
<p>请注意，z 也称为对数几率，因为 S 型函数的反函数表明，z 可定义为标签“1”（例如“狗叫”）的概率除以标签“0”（例如“狗不叫”）的概率得出的值的对数：</p>
<script type="math/tex; mode=display">z = log(\frac{y}{1-y})</script><h3 id="逻辑回归模型训练"><a href="#逻辑回归模型训练" class="headerlink" title="逻辑回归模型训练"></a>逻辑回归模型训练</h3><p>线性回归的损失函数是平方损失。逻辑回归的损失函数是对数损失函数，定义如下：</p>
<script type="math/tex; mode=display">Log Loss = \sum_{(x,y)\in D} -ylog(y') - (1 - y)log(1 - y')</script><p>其中：</p>
<ul>
<li>(xy)ϵD 是包含很多有标签样本 (x,y) 的数据集。</li>
<li>“y”是有标签样本中的标签。由于这是逻辑回归，因此“y”的每个值必须是 0 或 1。</li>
<li>“y’”是对于特征集“x”的预测值（介于 0 和 1 之间）。</li>
</ul>
<p>对数损失函数的方程式与 Shannon 信息论中的熵测量密切相关。它也是似然函数的负对数（假设“y”属于伯努利分布）。实际上，最大限度地降低损失函数的值会生成最大的似然估计值。</p>
<p>正则化在逻辑回归建模中极其重要。如果没有正则化，逻辑回归的渐近性会不断促使损失在高维度空间内达到 0。因此，大多数逻辑回归模型会使用以下两个策略之一来降低模型复杂性：</p>
<ul>
<li>L2 正则化。</li>
<li>早停法，即，限制训练步数或学习速率。</li>
</ul>
<h2 id="分类-1"><a href="#分类-1" class="headerlink" title="分类"></a>分类</h2><h3 id="指定阈值"><a href="#指定阈值" class="headerlink" title="指定阈值"></a>指定阈值</h3><p>为了将逻辑回归值映射到二元类别，您必须指定分类阈值（也称为判定阈值）。如果值高于该阈值，则表示“垃圾邮件”；如果值低于该阈值，则表示“非垃圾邮件”。人们往往会认为分类阈值应始终为 0.5，但阈值取决于具体问题，因此您必须对其进行调整。</p>
<h3 id="真与假以及正类别与负类别"><a href="#真与假以及正类别与负类别" class="headerlink" title="真与假以及正类别与负类别"></a>真与假以及正类别与负类别</h3><ul>
<li>真正例是指模型将正类别样本正确地预测为正类别。</li>
<li>真负例是指模型将负类别样本正确地预测为负类别。</li>
<li>假正例是指模型将负类别样本错误地预测为正类别</li>
<li>假负例是指模型将正类别样本错误地预测为负类别。</li>
</ul>
<h3 id="准确率"><a href="#准确率" class="headerlink" title="准确率"></a>准确率</h3><p>准确率是一个用于评估分类模型的指标。通俗来说，准确率是指我们的模型预测正确的结果所占的比例。正式点说，准确率的定义如下：</p>
<script type="math/tex; mode=display">\text{Accuracy} = \frac{\text{Number of correct predictions}}{\text{Total number of predictions}}</script><p>对于二元分类，也可以根据正类别和负类别按如下方式计算准确率：</p>
<script type="math/tex; mode=display">\text{Accuracy} = \frac{TP+TN}{TP+TN+FP+FN}</script><p>其中，TP = 真正例，TN = 真负例，FP = 假正例，FN = 假负例。</p>
<h3 id="精确率和召回率"><a href="#精确率和召回率" class="headerlink" title="精确率和召回率"></a>精确率和召回率</h3><p>当使用分类不平衡的数据集（比如正类别标签和负类别标签的数量之间存在明显差异）时，单单准确率一项并不能反映全面情况。这时需要能够更好地评估分类不平衡问题的指标：精确率和召回率。</p>
<p>精确率的定义如下：</p>
<script type="math/tex; mode=display">\text{Precision} = \frac{TP}{TP+FP}</script><p>从数学上讲，召回率的定义如下：</p>
<script type="math/tex; mode=display">\text{Recall} = \frac{TP}{TP+FN}</script><p>要全面评估模型的有效性，必须同时检查精确率和召回率。遗憾的是，精确率和召回率往往是此消彼长的情况。</p>
<h3 id="ROC-和曲线下面积"><a href="#ROC-和曲线下面积" class="headerlink" title="ROC 和曲线下面积"></a>ROC 和曲线下面积</h3><p>ROC 曲线（接收者操作特征曲线）是一种显示分类模型在所有分类阈值下的效果的图表。该曲线绘制了以下两个参数：</p>
<ul>
<li>真正例率</li>
<li>假正例率</li>
</ul>
<p>真正例率 (TPR) 是召回率的同义词，因此定义如下：</p>
<script type="math/tex; mode=display">TPR = \frac{TP} {TP + FN}</script><p>假正例率 (FPR) 的定义如下：</p>
<script type="math/tex; mode=display">FPR = \frac{FP} {FP + TN}</script><p>ROC 曲线用于绘制采用不同分类阈值时的 TPR 与 FPR。降低分类阈值会导致将更多样本归为正类别，从而增加假正例和真正例的个数。下图显示了一个典型的 ROC 曲线。</p>
<p></p><p align="center">
    <img src="images/ROCCurve.svg" width="90%" alt="不同分类阈值下的 TP 率与 FP 率">
</p><p></p>
<center>图8 不同分类阈值下的 TP 率与 FP 率</center>

<p>曲线下面积表示“ROC 曲线下面积”。也就是说，曲线下面积测量的是从 (0,0) 到 (1,1) 之间整个 ROC 曲线以下的整个二维面积（参考积分学）。</p>
<p></p><p align="center">
    <img src="images/AUC.svg" width="90%" alt="曲线下面积（ROC 曲线下面积）">
</p><p></p>
<center>图9 曲线下面积（ROC 曲线下面积）</center>

<p>曲线下面积对所有可能的分类阈值的效果进行综合衡量。曲线下面积的一种解读方式是看作模型将某个随机正类别样本排列在某个随机负类别样本之上的概率。</p>
<p>曲线下面积的取值范围为 0-1。预测结果 100% 错误的模型的曲线下面积为 0.0；而预测结果 100% 正确的模型的曲线下面积为 1.0。</p>
<p>曲线下面积因以下两个原因而比较实用：</p>
<ul>
<li>曲线下面积的尺度不变。它测量预测的排名情况，而不是测量其绝对值。</li>
<li>曲线下面积的分类阈值不变。它测量模型预测的质量，而不考虑所选的分类阈值。</li>
</ul>
<p>不过，这两个原因都有各自的局限性，这可能会导致曲线下面积在某些用例中不太实用：</p>
<ul>
<li><p>并非总是希望尺度不变。 例如，有时我们非常需要被良好校准的概率输出，而曲线下面积无法告诉我们这一结果。</p>
</li>
<li><p>并非总是希望分类阈值不变。 在假负例与假正例的代价存在较大差异的情况下，尽量减少一种类型的分类错误可能至关重要。例如，在进行垃圾邮件检测时，您可能希望优先考虑尽量减少假正例（即使这会导致假负例大幅增加）。对于此类优化，曲线下面积并非一个实用的指标。</p>
</li>
</ul>
<h2 id="预测偏差"><a href="#预测偏差" class="headerlink" title="预测偏差"></a>预测偏差</h2><p>逻辑回归预测应当无偏差。即:</p>
<script type="math/tex; mode=display">\text{预测平均值}\approx\text{观察平均值}</script><p>预测偏差指的是这两个平均值之间的差值。即：</p>
<script type="math/tex; mode=display">\text{预测偏差} = \text{预测平均值} - \text{数据集中相应标签的平均值}</script><p>造成预测偏差的可能原因包括：</p>
<ul>
<li>特征集不完整</li>
<li>数据集混乱</li>
<li>模型实现流水线中有错误？</li>
<li>训练样本有偏差</li>
<li>正则化过强</li>
</ul>
<h1 id="端到端（end-to-end）"><a href="#端到端（end-to-end）" class="headerlink" title="端到端（end to end）"></a>端到端（end to end）</h1><p>传统机器学习算法在应用过程中需要经历<strong>特征工程</strong>这一步骤，从研究对象中提取特征信息，便于后续的训练和测试。在传统机器学习中，<strong>特征工程</strong>非常重要，它提取特征的好坏关系到机器学习的最终效果。</p>
<p>本质上，<strong>特征工程</strong>是将研究对象信息降维的过程。而深度学习则无需这一手工提取特征的过程。以深度学习在图像分类中的应用为例，它直接输入高维的原始图像，输出即是图像分类。这个过程即叫做端到端。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a href="https://zh.wikipedia.org/wiki/机器学习" target="_blank" rel="noopener">机器学习</a>，by wikipedia.</li>
<li><a href="https://developers.google.com/machine-learning/crash-course/ml-intro?hl=zh-cn" target="_blank" rel="noopener">机器学习速成课程</a>，by google.</li>
<li><a href="https://www.zybuluo.com/knight/note/96093" target="_blank" rel="noopener">MathJax使用LaTeX语法编写数学公式教程</a></li>
<li><a href="https://www.zhihu.com/question/51435499" target="_blank" rel="noopener">什么是 end-to-end 神经网络？</a>,by zhihu.</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangwang.github.io/2018/11/19/计算机视觉之相机成像原理与坐标系转换/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Huang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Huang's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/19/计算机视觉之相机成像原理与坐标系转换/" itemprop="url">
                  计算机视觉之相机成像原理与坐标系转换
                </a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-19 16:02:41" itemprop="dateCreated datePublished" datetime="2018-11-19T16:02:41+08:00">2018-11-19</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-19 21:15:10" itemprop="dateModified" datetime="2019-05-19T21:15:10+08:00">2019-05-19</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>计算机视觉是一门研究用摄影机和计算机代替人眼对目标进行识别、跟踪和测量的学科。为了解该门学科，首先应掌握投影原理和世界坐标系、相机坐标系、图像坐标系、像素坐标系之间的转换关系。</p>
<h1 id="三维投影"><a href="#三维投影" class="headerlink" title="三维投影"></a>三维投影</h1><p>计算机3D图形学中，三维投影是将三维空间中的点映射到二维平面上的方法。常用三维投影有正交投影和透视投影。正交投影通常用于对现实物品的三维建模，而透视投影与人的视觉系统类似，常用于在二维平面呈现三维世界。</p>
<h2 id="正交投影原理"><a href="#正交投影原理" class="headerlink" title="正交投影原理"></a>正交投影原理</h2><p>正交投影是一系列用于显示三维物体的轮廓、细节或精确测量结果的变换方法。通常又称作截面图、鸟瞰图或立面图。</p>
<p>当视平面的法向（即摄像机的朝向）平行于笛卡尔坐标系三根坐标轴中的一根，数学变换定义如下： 若使用一个平行于y轴（侧视图）的正交投影将三维点 $a<em>{x}$, $a</em>{y}$,$a<em>{z}$投影到二维平面上得到二维点 $b</em>{x}$,$b_{y}$，可以使用如下公式</p>
<script type="math/tex; mode=display">b_x=s_xa_x+c_x</script><script type="math/tex; mode=display">b_y=s_za_z+c_z</script><p>其中向量s是一个任意的缩放因子，而c是一个任意的偏移量。这些常量可自由选择，通常用于将视口调整到一个合适的位置。该投影变换同样可以使用矩阵表示（为清晰起见引入临时向量d）</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
    d_x  \\
    d_y  \\
\end{bmatrix}
=
\begin{bmatrix}
    1 & 0 & 0 \\
    0 & 0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
    a_x \\
    a_y \\
    a_z \\
\end{bmatrix}</script><script type="math/tex; mode=display">
\begin{bmatrix}
    b_x\\
    b_y\\
\end{bmatrix}
=
\begin{bmatrix}
    s_x & 0 \\
    0     & s_z \\
\end{bmatrix}
\begin{bmatrix}
    d_x\\
    d_y\\
\end{bmatrix}
+
\begin{bmatrix}
    c_x\\
    c_z\\
\end{bmatrix}</script><p>虽然正交投影产生的图像在一定程度上反映了物体的三维特性，但此类投影图像和实际观测到的并不相同。特别是对于相同长度的平行线段，无论离虚拟观察者（摄像机）远近与否，它们都会在正交投影中显示为相同长度。这会导致较近的线段看起来被缩短了。</p>
<h2 id="透视投影原理"><a href="#透视投影原理" class="headerlink" title="透视投影原理"></a>透视投影原理</h2><p>透视投影是为了获得接近真实三维物体的视觉效果而在二维的纸或者画布平面上绘图或者渲染的一种方法，它也称为透视图。透视投影的绘制必须根据已有的几何规则进行。</p>
<p>常用的透视投影视椎体模型如图1所示。设视点E位于原点，视平面P垂直于Z轴，且四边分别平行于x轴和y轴，视椎体的近截面离视点的距离为n，远截面离视点的距离为f，且一般取近截面为视平面。</p>
<p></p><p align="center">
    <img src="images/perspective_model.jpg" width="90%" alt="透视投影的标准视椎体模型">
</p><p></p>
<center>图1 透视投影的标准视椎体模型</center>

<h1 id="坐标系之间的转换"><a href="#坐标系之间的转换" class="headerlink" title="坐标系之间的转换"></a>坐标系之间的转换</h1><p>计算机视觉通常涉及到四个坐标系：像素平面坐标系（u,v）、像平面坐标系（图像物理坐标第（x,y）、相机坐标系（Xc,Yc,Zc）和世界坐标系（Xw,Yw,Zw），如图2所示。</p>
<p></p><p align="center">
    <img src="images/four_axis.png" width="90%" alt="四个坐标系">
</p><p></p>
<center>图2 四个坐标系</center>

<p>1 : 世界坐标系：根据情况而定，可以表示任何物体。单位m。</p>
<p>2：相机坐标系：以摄像机光心为原点（在针孔模型中也就是针孔为光心），z轴与光轴重合也就是z轴指向相机的前方（也就是与成像平面垂直），x轴与y轴的正方向与物体坐标系平行，其中上图中的f为摄像机的焦距。单位m</p>
<p>3：图像物理坐标系（也叫平面坐标系）：用物理单位表示像素的位置，坐标原点为摄像机光轴与图像物理坐标系的交点位置。坐标系为图上o-xy。单位是mm。单位毫米的原因是此时由于相机内部的CCD传感器是很小的，比如8mm x 6mm。但是最后图像照片是也像素为单位比如640x480.这就涉及到了图像物理坐标系与像素坐标系的变换了。下面的像素坐标系将会讲到。</p>
<p>4：像素坐标系：以像素为单位，坐标原点在左上角。这也是一些opencv，OpenGL等库的坐标原点选在左上角的原因。当然明显看出CCD传感器以mm单位到像素中间有转换的。举个例子，CCD传感上上面的8mm x 6mm，转换到像素大小是640x480. 假如dx表示像素坐标系中每个像素的物理大小就是1/80. 也就是说毫米与像素点的之间关系是piexl/mm.</p>
<h2 id="世界坐标系到相机坐标系的转换"><a href="#世界坐标系到相机坐标系的转换" class="headerlink" title="世界坐标系到相机坐标系的转换"></a>世界坐标系到相机坐标系的转换</h2><p>物体之间的坐标系变换都可以表示坐标系的旋转变换加上平移变换，则世界坐标系到相机坐标系的转换关系也是如此，他们之间的变换如图3所示。</p>
<p></p><p align="center">
    <img src="images/worldtocamera.jpg" width="90%" alt="世界坐标系到相机坐标系的转换">
</p><p></p>
<center>图3 世界坐标系到相机坐标系的转换</center>

<p>可以得到P点在相机坐标系下的坐标:</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
    X_c\\
    Y_c\\
    Z_c\\
\end{bmatrix}
=
R
\begin{bmatrix}
    X_w\\
    Y_w\\
    Z_w\\
\end{bmatrix}
+T
\Rightarrow 
\begin{bmatrix}
    X_c\\
    Y_c\\
    Z_c\\
    1\\
\end{bmatrix}
=
\begin{bmatrix}
    R & T\\
    \vec{0} & 1\\
\end{bmatrix}
\begin{bmatrix}
    X_w\\
    Y_w\\
    Z_w\\
    1\\
\end{bmatrix}
,
R:3*3,T:3*1</script><h2 id="相机坐标系到图像物理坐标系的转换"><a href="#相机坐标系到图像物理坐标系的转换" class="headerlink" title="相机坐标系到图像物理坐标系的转换"></a>相机坐标系到图像物理坐标系的转换</h2><p>从相机坐标系到图像坐标系，属于透视投影关系，从3D转换到2D。 也可以看成是针孔模型的变种。该转换满足三角形的相似定理，如图4所示。</p>
<p></p><p align="center">
    <img src="images/3dto2d.jpg" width="90%" alt="相机坐标系到图像物理坐标系的转换">
</p><p></p>
<center>图4 相机坐标系到图像物理坐标系的转换</center>

<h2 id="图像物理坐标系到像素坐标系的转换"><a href="#图像物理坐标系到像素坐标系的转换" class="headerlink" title="图像物理坐标系到像素坐标系的转换"></a>图像物理坐标系到像素坐标系的转换</h2><p>图像物理坐标系到像素坐标系的转换不涉及旋转变换，但是坐标原点位置不一致，大小不一致，涉及伸缩变换及平移变换，如图5所示。</p>
<p></p><p align="center">
    <img src="images/image2uv.jpg" width="90%" alt="图像物理坐标系到像素坐标系的转换">
</p><p></p>
<center>图5 图像物理坐标系到像素坐标系的转换</center>

<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>四个坐标系之间存在着下述关系 ( 矩阵依次左乘 )，如图6所示:</p>
<p></p><p align="center">
    <img src="images/summary1.jpg" width="90%" alt="世界坐标系到像素坐标系的转换">
</p><p></p>
<p></p><p align="center">
    <img src="images/summary2.jpg" width="90%">
</p><p></p>
<center>图6 世界坐标系到像素坐标系的转换</center>

<p>其中相机的内参和外参可以通过张正友标定获取。通过最终的转换关系来看，一个三维中的坐标点，的确可以在图像中找到一个对应的像素点，但是反过来，通过图像中的一个点找到它在三维中对应的点就很成了一个问题，因为我们并不知道等式左边的Z<sub>c</sub>的值。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a href="https://zh.wikipedia.org/wiki/三维投影" target="_blank" rel="noopener">三维投影</a>,by wikipedia.</li>
<li><a href="https://blog.csdn.net/Goncely/article/details/5397729" target="_blank" rel="noopener">透视投影的原理和实现</a>,by Goncely.</li>
<li><a href="https://blog.csdn.net/chentravelling/article/details/53558096" target="_blank" rel="noopener">计算机视觉：相机成像原理：世界坐标系、相机坐标系、图像坐标系、像素坐标系之间的转换</a>,by 生活没有if-else</li>
<li><a href="https://blog.csdn.net/lyl771857509/article/details/79633412" target="_blank" rel="noopener">【相机标定】四个坐标系之间的变换关系</a></li>
<li><a href="http://zhaoxuhui.top/blog/2018/03/18/Location&amp;PoseEstimationInSLAM.html" target="_blank" rel="noopener">SLAM相机位姿估计(1)</a>,by Zhao xuhui.</li>
<li><a href="https://zhaoxuhui.top/blog/2018/03/08/%E5%8D%95%E7%9B%AESLAM%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80.html" target="_blank" rel="noopener">单目SLAM理论基础</a>,by Zhao xuhui.</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangwang.github.io/2018/11/17/生活中的统计学之购买车展黄牛票/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Huang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Huang's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/17/生活中的统计学之购买车展黄牛票/" itemprop="url">
                  生活中的统计学之购买车展黄牛票
                </a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-17 18:54:07" itemprop="dateCreated datePublished" datetime="2018-11-17T18:54:07+08:00">2018-11-17</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-23 09:25:12" itemprop="dateModified" datetime="2019-06-23T09:25:12+08:00">2019-06-23</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>前段时间去看车展，一出地铁口，就有黄牛兜售车展门票，只需30元一张，而从车展正规窗口购买需要50元一张，那么买黄牛票还是买正规车展门票呢？</p>
<p>黄牛票有可能是真的，这样我就只需30元就可以参观车展，也有可能是假的，这样我就得花80元才能参观车展。假设黄牛票为真的概率是p<sub>1</sub>，根据概率论的知识，我参观车展花费的期望是：</p>
<script type="math/tex; mode=display">E = 30*p_1+80*(1-p_1)</script><p>在没有任何先验知识的前提下，假设黄牛票为真的概率是0.5，于是每次买黄牛票参观车展的花费期望是55，而每次买正规门票参观车展的花费期望是50，因此不建议买黄牛票，而应该去买正规车展门票。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a href="https://www.zybuluo.com/codeep/note/163962" target="_blank" rel="noopener">Cmd Markdown 公式指导手册</a></li>
<li><a href="http://jzqt.github.io/2015/06/30/Markdown中写数学公式/" target="_blank" rel="noopener">Markdown中写数学公式</a></li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangwang.github.io/2018/11/11/中国电信光猫华为HG8245C开启IPV6的方法/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Huang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Huang's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/11/中国电信光猫华为HG8245C开启IPV6的方法/" itemprop="url">
                  中国电信光猫华为HG8245C开启IPV6的方法
                </a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-11 20:50:15" itemprop="dateCreated datePublished" datetime="2018-11-11T20:50:15+08:00">2018-11-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-04-04 20:32:05" itemprop="dateModified" datetime="2019-04-04T20:32:05+08:00">2019-04-04</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>据报道，目前中国电信已成功创建了IP骨干网全面支持IPv6，并且在4G网络开启了IPv6服务，在100多个城域网提供了IPv6服务<sup>[1]</sup>。那么如何使家里宽带用上IPv6服务呢？这个问题最关键是设置入户光猫使其支持IPv6。以如何光猫华为HG8245C为例，说明设置过程。</p>
<h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h2 id="IPv6简介"><a href="#IPv6简介" class="headerlink" title="IPv6简介"></a>IPv6简介</h2><p>网际协议第6版（英文：Internet Protocol version 6，缩写：IPv6）是网际协议（IP）的最新版本，用作互联网的网上层协议，用它来取代IPv4主要是为了解决IPv4地址枯竭问题，不过它也在其他很多方面对IPv4有所改进。</p>
<h2 id="IPv6格式"><a href="#IPv6格式" class="headerlink" title="IPv6格式"></a>IPv6格式</h2><p>IPv6二进位制下为128位长度，以16位为一组，每组以冒号“:”隔开，可以分为8组，每组以4位十六进制方式表示。例如：2001:0db8:85a3:08d3:1319:8a2e:0370:7344 是一个合法的IPv6地址。</p>
<p>同时IPv6在某些条件下可以省略：</p>
<ol>
<li><p>每项数字前导的0可以省略，省略后前导数字仍是0则继续，例如下组IPv6是等价的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2001:0DB8:02de:0000:0000:0000:0000:0e13</span><br><span class="line">2001:DB8:2de:0000:0000:0000:0000:e13</span><br><span class="line">2001:DB8:2de:000:000:000:000:e13</span><br><span class="line">2001:DB8:2de:00:00:00:00:e13</span><br><span class="line">2001:DB8:2de:0:0:0:0:e13</span><br></pre></td></tr></table></figure>
</li>
<li><p>可以用双冒号“::”表示一组0或多组连续的0，但只能出现一次。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">* 2001:DB8:2de:0:0:0:0:e13</span><br><span class="line">   2001:DB8:2de::e13</span><br><span class="line">* 2001:0DB8:0000:0000:0000:0000:1428:57ab</span><br><span class="line">   2001:0DB8:0000:0000:0000::1428:57ab</span><br><span class="line">   2001:0DB8:0:0:0:0:1428:57ab</span><br><span class="line">   2001:0DB8:0::0:1428:57ab</span><br><span class="line">   2001:0DB8::1428:57ab</span><br></pre></td></tr></table></figure>
</li>
<li><p>如果这个地址实际上是IPv4的地址，后32位可以用10进制数表示；因此::ffff:192.168.89.9 相等于::ffff:c0a8:5909。</p>
</li>
</ol>
<h2 id="IPv6地址分类"><a href="#IPv6地址分类" class="headerlink" title="IPv6地址分类"></a>IPv6地址分类</h2><h3 id="常用地址"><a href="#常用地址" class="headerlink" title="常用地址"></a>常用地址</h3><p>IPv6地址可分为三种:</p>
<ul>
<li>单播（unicast）地址：单播地址标示一个网上接口。协议会把送往地址的数据包送往给其接口。</li>
<li>任播（anycast）地址: Anycast是IPv6特有的数据发送方式，它像是IPv4的Unicast（单点传播）与Broadcast（多点广播）的综合。</li>
<li>多播（multicast）地址: 多播地址也称组播地址。多播地址也被指定到一群不同的接口，送到多播地址的数据包会被发送到所有的地址。</li>
</ul>
<h3 id="特殊地址"><a href="#特殊地址" class="headerlink" title="特殊地址"></a>特殊地址</h3><h4 id="未指定地址"><a href="#未指定地址" class="headerlink" title="未指定地址"></a>未指定地址</h4><ul>
<li>::/128－所有比特皆为零的地址称作未指定地址。</li>
</ul>
<h4 id="链路本地地址"><a href="#链路本地地址" class="headerlink" title="链路本地地址"></a>链路本地地址</h4><ul>
<li>::1/128－是一种单播绕回地址。如果一个应用程序将数据包送到此地址，IPv6堆栈会转送这些数据包绕回到同样的虚拟接口（相当于IPv4中的127.0.0.1/8）。</li>
<li>fe80::/10－这些链路本地地址指明，这些地址只在区域连线中是合法的，这有点类似于IPv4中的169.254.0.0/16。</li>
</ul>
<h4 id="唯一区域位域"><a href="#唯一区域位域" class="headerlink" title="唯一区域位域"></a>唯一区域位域</h4><ul>
<li>fc00::/7－唯一区域地址（ULA，unique local address）只可在一群网站中绕送。</li>
</ul>
<h4 id="多播地址"><a href="#多播地址" class="headerlink" title="多播地址"></a>多播地址</h4><ul>
<li>ff00::/8－这个前置表明定义在”IP Version 6 Addressing Architecture”（RFC 4291）中的多播地址[</li>
</ul>
<h4 id="IPv4转译地址"><a href="#IPv4转译地址" class="headerlink" title="IPv4转译地址"></a>IPv4转译地址</h4><ul>
<li>::ffff:x.x.x.x/96－用于IPv4映射地址。</li>
<li>2001::/32－用于Teredo隧道。</li>
<li>2002::/16－用于6to4。</li>
</ul>
<h2 id="IPv6优势"><a href="#IPv6优势" class="headerlink" title="IPv6优势"></a>IPv6优势</h2><ul>
<li>巨大的地址空间</li>
<li>新的协议头格式，加快路由速度</li>
<li>有效地、分级的寻址和路由结构</li>
<li>有状态和无状态的地址配置</li>
<li>内置的安全性</li>
<li>更好的支持Qos</li>
<li>用新协议处理邻节点的交互</li>
<li>可扩展性</li>
</ul>
<h1 id="设置华为光猫HG8245C开启IPv6步骤"><a href="#设置华为光猫HG8245C开启IPv6步骤" class="headerlink" title="设置华为光猫HG8245C开启IPv6步骤"></a>设置华为光猫HG8245C开启IPv6步骤</h1><p>在设置华为光猫HG8245C开启IPv6之前，一是要确保所在电信已支持IPv6服务。二是获取华为HG8245C隐藏管理员帐号telecomadmin的密码，通常为<strong>nE7jA%5m</strong>。</p>
<p>以隐藏管理员帐号telecomadmin登录华为HG8245C的Web管理控制台后，选择“网络-&gt;宽带设置”，选择连接”2<em>INTERNET_R_VID</em>“,设置协议类型为“IPv4/IPv6”，设置前缀获取方式为“DHCPv6-PD”,再点应用即可。</p>
<p align="center">
    <img src="images/open_ipv6.png" width="90%" alt="开启IPv6">
</p>

<p>在Debian Linux中打开终端，输入如下命令或者浏览网站<a href="http://test-ipv6.com/验证IPv6。" target="_blank" rel="noopener">http://test-ipv6.com/验证IPv6。</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ ip addr</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: wlan0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000</span><br><span class="line">    link/ether e4:70:b8:30:f1:5b brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.1.100/24 brd 192.168.1.255 scope global dynamic noprefixroute wlan0</span><br><span class="line">       valid_lft 251566sec preferred_lft 251566sec</span><br><span class="line">    inet6 240e:bc:e60:3d00:ea5b:f704:6b65:fab1/64 scope global dynamic noprefixroute </span><br><span class="line">       valid_lft 258984sec preferred_lft 172584sec</span><br><span class="line">    inet6 fe80::4e2c:4397:f016:3eb4/64 scope link noprefixroute </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure></p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a href="http://www.c114.com.cn/news/117/a1059059.html" target="_blank" rel="noopener">中国电信：IPv6在线用户已超千万 年底将完成端到端服务能力</a>.2018-07-16</li>
<li>维基百科.<a href="https://zh.wikipedia.org/wiki/IPv6" target="_blank" rel="noopener">IPv6</a></li>
<li><a href="http://network.51cto.com/art/201006/204898.htm" target="_blank" rel="noopener">掌握IPv6网络协议的优势</a>,2010-06-10</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://huangwang.github.io/2018/11/09/树莓派学习之SFTP管理文件/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jack Huang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jack Huang's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/09/树莓派学习之SFTP管理文件/" itemprop="url">
                  树莓派学习之SFTP管理文件
                </a>
              
            
          </h2>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-09 21:53:11 / 修改时间：22:39:13" itemprop="dateCreated datePublished" datetime="2018-11-09T21:53:11+08:00">2018-11-09</time>
            

            
              

              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>当需要向树莓派发送文件时，可使用SFTP上传下载文件。下面介绍如何使用SFTP向树莓派发送下载文件。</p>
<h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><p>SFTP是Secure File Transfer Protocol的缩写，安全文件传送协议。可以为传输文件提供一种安全的网络的加密方法。sftp 与 ftp 有着几乎一样的语法和功能。</p>
<p>SFTP 为 SSH的其中一部分，是一种传输文件至服务器的安全方式。在SSH软件包中，已经包含了一个叫作SFTP(Secure File Transfer Protocol)的安全文件信息传输子系统，SFTP本身没有单独的守护进程，它必须使用sshd守护进程（端口号默认是22）来完成相应的连接和答复操作。</p>
<p>SFTP传输使用了加密/解密技术，所以传输效率比普通的FTP要低得多，如果您对网络安全性要求更高时，可以使用SFTP代替FTP。</p>
<h2 id="SFTP客户端程序"><a href="#SFTP客户端程序" class="headerlink" title="SFTP客户端程序"></a>SFTP客户端程序</h2><p>在Windows平台，常用的SFTP客户端程序有：</p>
<ul>
<li>FileZilla</li>
<li>WinSCP</li>
<li>Xftp</li>
<li>Core FTP</li>
</ul>
<p>在Linux平台，可直接使用sftp命令进行连接服务器。</p>
<h1 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h1><p>下面介绍在Debian平台使用sftp命令连接树莓派，<strong>在此之前应配置树莓派开启ssh服务</strong>。</p>
<ol>
<li>建立连接</li>
</ol>
<p>使用如下命令连接树莓派：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sftp pi@192.168.0.103</span><br></pre></td></tr></table></figure></p>
<ol>
<li>查看帮助</li>
</ol>
<p>通过help查看在sftp连接下能使用的命令。从帮助中可知，在命令前加前缀“l”或者“！”即可在本地操作系统shell执行命令。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">sftp&gt; help</span><br><span class="line">Available commands:</span><br><span class="line">bye                                Quit sftp</span><br><span class="line">cd path                            Change remote directory to &apos;path&apos;</span><br><span class="line">chgrp grp path                     Change group of file &apos;path&apos; to &apos;grp&apos;</span><br><span class="line">chmod mode path                    Change permissions of file &apos;path&apos; to &apos;mode&apos;</span><br><span class="line">chown own path                     Change owner of file &apos;path&apos; to &apos;own&apos;</span><br><span class="line">df [-hi] [path]                    Display statistics for current directory or</span><br><span class="line">                                   filesystem containing &apos;path&apos;</span><br><span class="line">exit                               Quit sftp</span><br><span class="line">get [-afPpRr] remote [local]       Download file</span><br><span class="line">reget [-fPpRr] remote [local]      Resume download file</span><br><span class="line">reput [-fPpRr] [local] remote      Resume upload file</span><br><span class="line">help                               Display this help text</span><br><span class="line">lcd path                           Change local directory to &apos;path&apos;</span><br><span class="line">lls [ls-options [path]]            Display local directory listing</span><br><span class="line">lmkdir path                        Create local directory</span><br><span class="line">ln [-s] oldpath newpath            Link remote file (-s for symlink)</span><br><span class="line">lpwd                               Print local working directory</span><br><span class="line">ls [-1afhlnrSt] [path]             Display remote directory listing</span><br><span class="line">lumask umask                       Set local umask to &apos;umask&apos;</span><br><span class="line">mkdir path                         Create remote directory</span><br><span class="line">progress                           Toggle display of progress meter</span><br><span class="line">put [-afPpRr] local [remote]       Upload file</span><br><span class="line">pwd                                Display remote working directory</span><br><span class="line">quit                               Quit sftp</span><br><span class="line">rename oldpath newpath             Rename remote file</span><br><span class="line">rm path                            Delete remote file</span><br><span class="line">rmdir path                         Remove remote directory</span><br><span class="line">symlink oldpath newpath            Symlink remote file</span><br><span class="line">version                            Show SFTP version</span><br><span class="line">!command                           Execute &apos;command&apos; in local shell</span><br><span class="line">!                                  Escape to local shell</span><br><span class="line">?                                  Synonym for help</span><br></pre></td></tr></table></figure></p>
<ol>
<li>下载远程文件到本地主机<br>使用get命令下载远程文件到本地主机：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sftp&gt; get README.TXT </span><br><span class="line">Fetching /home/pi/wiringPi/README.TXT to README.TXT</span><br><span class="line">/home/pi/wiringPi/README.TXT                  100%  606    39.1KB/s   00:00</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>get命令还有一些有用参数，如递归选项“ -r ”来递归的复制一个文件夹里面的内容，“ -P ”或者“ -p ”参数来告诉 SFTP 保持文件的权限访问位的设置和访问时间。</p>
<ol>
<li>上传本地文件到远程主机<br>使用“ put ”命令将文件上传到远程主机：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sftp&gt; put README.TXT </span><br><span class="line">Uploading README.TXT to /home/pi/README.TXT</span><br><span class="line">README.TXT                                    100%  606    33.6KB/s   00:00</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>” put “具有类似“ get ”的参数。例如，递归选项“ -r ”可以上传整个文件夹。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol>
<li>华华. <a href="https://linuxstory.org/how-to-use-sftp-to-securely-transfer-files-with-a-remote-server/" target="_blank" rel="noopener">手把手教你使用 SFTP 安全地传输文件</a>.2015-12-5.</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/11/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><span class="page-number current">12</span><a class="page-number" href="/page/13/">13</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/13/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Jack Huang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">167</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">52</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          
            
          
          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jack Huang</span>

  

  
</div>


  



  <div class="powered-by">由 <a class="theme-link" target="_blank" rel="external nofollow" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" rel="external nofollow" href="https://github.com/theme-next/hexo-theme-next">NexT.Muse</a></div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="总访客量">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="site-pv" title="总访问量">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.2.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.2.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.2.0"></script>



  



	





  





  






<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    

    






  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    
      
    
      
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  
  

  

  

  

  

  

</body>
</html>
